#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è§†é¢‘æ€»ç»“åº”ç”¨ - å®Œæ•´çš„YouTube/Bilibiliè§†é¢‘æ€»ç»“å·¥å…·
è¾“å…¥è§†é¢‘é“¾æ¥ï¼Œä¸‹è½½è§†é¢‘å’Œå­—å¹•ï¼Œç”ŸæˆAIæ€»ç»“å¹¶é™„å¸¦æˆªå›¾
"""

import os
import sys
import re
import subprocess
import json
import logging
from datetime import datetime
from bisect import bisect_left
from urllib.parse import quote
from typing import List, Dict, Any, Tuple
from concurrent.futures import ThreadPoolExecutor
import cv2
import numpy as np

try:
    from google import genai
except ImportError:
    genai = None

# ==== LLM é…ç½®ï¼ˆå¯æ ¹æ®éœ€è¦ä¿®æ”¹ï¼‰====
# os.environ.get("GEMINI_API_KEY", "")
GEMINI_API_KEY = ""
PRIMARY_MODEL = "gemini-2.5-pro"
BASE_SYSTEM_PROMPT = """
## Background Information

You are an expert proficient in computer graphics, skilled at explaining complex technical concepts in a clear, structured manner to people who have a foundation in graphics but wish to delve deeper into the content of a video lecture.
You are reading through and summarizing a long technical graphics lecture transcript section by section. This is part ${current}$ of ${total}$ total parts.
My Goal: I am a **Game Engine Engineer and Rendering Engineer** with a **graphics background**. I aim to enrich my knowledge, gain in-depth mastery of graphics and game engine knowledge, and understand industry developments, thus seeking to **deeply study the content related to the lecture**.

## Task Requirements

1.  **Core Task:** Please summarize the content I provide below into **easily understandable and memorable notes**.
2.  **Formatting Requirements:**
    * Use a **clear hierarchical structure** (main headings, subheadings, bullet points).
    * For each topic, distill and **bold** the **core concepts** and **key terminology**.
    * If **formulas or important algorithms** are involved, please highlight them. Use standard **LaTeX format** to ensure the document can be rendered correctly.
    * The language style should be as **accessible as possible**, avoiding direct copying of obscure jargon from the original text.
    * **Strictly No Meta-talk:**
        * **Absolutely prohibited** to output any opening or closing remarks (e.g., "...Here are the notes prepared for you...").
        * **Absolutely prohibited** to include metadata titles like "Part ${current}$" or "Part X" in the body text.
        * **Start directly with the technical content title.**
3. è¾“å‡ºä¸­æ–‡ï¼
## Content to be Summarized
"""

# ==== å­—å¹•è§£æé€»è¾‘====
SubtitleEntry = Dict[str, str]
SubtitleData = List[SubtitleEntry]


def parse_subtitles(file_content: str) -> Tuple[SubtitleData, str]:
    """
    è§£æ SRT/VTT å­—å¹•ï¼Œè¿”å›ç»“æ„åŒ–å­—å¹•åˆ—è¡¨ä¸æ•´åˆæ–‡æœ¬
    """
    if file_content.startswith('\ufeff'):
        file_content = file_content.lstrip('\ufeff')
    if file_content.startswith('WEBVTT'):
        file_content = re.sub(r'WEBVTT.*?\n\n', '',
                              file_content, flags=re.DOTALL)

    blocks = file_content.strip().split('\n\n')
    subtitle_data: SubtitleData = []
    consolidated_lines: List[str] = []
    timestamp_pattern = re.compile(
        r'(\d{2}:\d{2}:\d{2}[,\.]\d{3})\s*-->\s*(\d{2}:\d{2}:\d{2}[,\.]\d{3})')

    for block in blocks:
        lines = block.strip().split('\n')
        if not lines:
            continue

        time_match = None
        dialogue_lines: List[str] = []

        for line in lines:
            line = line.strip()
            if '-->' in line and timestamp_pattern.search(line):
                time_match = timestamp_pattern.search(line)
            elif line.isdigit():
                continue
            else:
                dialogue_lines.append(line)

        if time_match and dialogue_lines:
            start_time = time_match.group(1).replace('.', ',')
            end_time = time_match.group(2).replace('.', ',')
            full_dialogue = ' '.join(dialogue_lines)
            subtitle_data.append({
                'start': start_time,
                'end': end_time,
                'text': full_dialogue
            })
            consolidated_lines.append(full_dialogue)

    consolidated_text = '\n'.join(consolidated_lines)
    return subtitle_data, consolidated_text


def detect_language(content: str, chinese_threshold: float = 0.1) -> str:
    """æ£€æµ‹æ–‡æœ¬ä¸»è¦è¯­è¨€ï¼Œé»˜è®¤ä¸­æ–‡å­—ç¬¦æ¯”ä¾‹â‰¥é˜ˆå€¼è§†ä¸ºä¸­æ–‡"""
    total_chars = len(content)
    if total_chars == 0:
        return "Unknown"

    chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', content))
    chinese_ratio = chinese_chars / total_chars
    language = "Chinese" if chinese_ratio >= chinese_threshold else "English"
    logger.info(f"ğŸŒ æ£€æµ‹è¯­è¨€: {language} (ä¸­æ–‡æ¯”ä¾‹: {chinese_ratio:.2%})")
    return language


def generate_chunk_summary(client, chunk_text: str, current_idx: int,
                           total_chunks: int, model_name: str = PRIMARY_MODEL) -> str:
    """
    è°ƒç”¨ Gemini æ¨¡å‹ç”Ÿæˆå•ä¸ªç‰‡æ®µçš„æ€»ç»“
    """
    if client is None:
        raise RuntimeError("genai client æœªåˆå§‹åŒ–")

    prompt = BASE_SYSTEM_PROMPT.format(
        current=current_idx, total=total_chunks) + "\n\n" + chunk_text

    logger.info(
        f"   >>> LLM æ€»ç»“ç¬¬ {current_idx}/{total_chunks} ç‰‡æ®µï¼Œé•¿åº¦ {len(chunk_text)} å­—")
    response = client.models.generate_content(
        model=model_name,
        contents=prompt,
    )
    return response.text


# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class VideoDownloader:
    """è§†é¢‘å’Œå­—å¹•ä¸‹è½½å™¨ï¼ˆä½¿ç”¨yt-dlpï¼‰"""

    def __init__(self, output_dir: str = "downloads", cookies_file: str = None):
        """
        åˆå§‹åŒ–ä¸‹è½½å™¨

        Args:
            output_dir: ä¸‹è½½æ–‡ä»¶ä¿å­˜ç›®å½•
            cookies_file: Cookies æ–‡ä»¶è·¯å¾„ï¼ˆç”¨äº Bilibili ç­‰éœ€è¦ç™»å½•çš„ç½‘ç«™ï¼‰
        """
        self.output_dir = output_dir
        self.cookies_file = cookies_file
        os.makedirs(output_dir, exist_ok=True)

    def _build_ytdlp_command(self, base_args: List[str]) -> List[str]:
        """
        æ„å»º yt-dlp å‘½ä»¤ï¼Œè‡ªåŠ¨æ·»åŠ  cookies å‚æ•°ï¼ˆå¦‚æœæä¾›ï¼‰

        Args:
            base_args: yt-dlp çš„åŸºç¡€å‚æ•°åˆ—è¡¨ï¼ˆä¸åŒ…å« 'yt-dlp'ï¼‰

        Returns:
            å®Œæ•´çš„å‘½ä»¤åˆ—è¡¨
        """
        cmd = ['yt-dlp']
        if self.cookies_file:
            if os.path.exists(self.cookies_file):
                cmd.extend(['--cookies', self.cookies_file])
                logger.info(f"ğŸª ä½¿ç”¨ cookies æ–‡ä»¶: {self.cookies_file}")
            else:
                logger.warning(
                    f"âš ï¸  Cookies æ–‡ä»¶ä¸å­˜åœ¨: {self.cookies_file}ï¼Œå°†ä¸ä½¿ç”¨ cookies")
        cmd.extend(base_args)
        return cmd

    def download(self, url: str) -> Dict[str, str]:
        """
        ä¸‹è½½è§†é¢‘å’Œå­—å¹•

        Args:
            url: è§†é¢‘é“¾æ¥ï¼ˆYouTube/Bilibiliç­‰ï¼‰

        Returns:
            åŒ…å«è§†é¢‘è·¯å¾„å’Œå­—å¹•è·¯å¾„çš„å­—å…¸
            {
                'video': 'è§†é¢‘æ–‡ä»¶è·¯å¾„',
                'subtitle': 'å­—å¹•æ–‡ä»¶è·¯å¾„' æˆ– None,
                'title': 'è§†é¢‘æ ‡é¢˜'
            }
        """
        logger.info(f"å¼€å§‹ä¸‹è½½: {url}")

        # æ£€æŸ¥yt-dlpæ˜¯å¦å®‰è£…
        try:
            subprocess.run(['yt-dlp', '--version'],
                           capture_output=True, check=True)
        except (subprocess.CalledProcessError, FileNotFoundError):
            logger.error("é”™è¯¯: æœªæ‰¾åˆ° yt-dlpï¼Œè¯·å…ˆå®‰è£…: pip install yt-dlp")
            raise

        # è®¾ç½®è¾“å‡ºæ¨¡æ¿
        video_template = os.path.join(self.output_dir, '%(title)s.%(ext)s')
        subtitle_template = os.path.join(
            self.output_dir, '%(title)s.%(language)s.%(ext)s')

        # é¦–å…ˆè·å–è§†é¢‘ä¿¡æ¯
        info_cmd = self._build_ytdlp_command([
            '--dump-json',
            '--skip-download',
            url
        ])

        try:
            info_output = subprocess.run(
                info_cmd, capture_output=True, text=True, check=True
            )
            video_info = json.loads(info_output.stdout)
            video_title = video_info.get('title', 'video')
            logger.info(f"ğŸ“¹ æ£€æµ‹åˆ°è§†é¢‘æ ‡é¢˜: {video_title}")
            # æ¸…ç†æ ‡é¢˜ä¸­çš„éæ³•å­—ç¬¦
            video_title = re.sub(r'[<>:"/\\|?*]', '_', video_title)
            logger.info(f"ğŸ“ æ¸…ç†åçš„æ ‡é¢˜: {video_title}")
        except Exception as e:
            logger.warning(f"è·å–è§†é¢‘ä¿¡æ¯å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤æ ‡é¢˜")
            video_title = 'video'

        # æ£€æŸ¥æœ¬åœ°æ˜¯å¦å·²æœ‰è§†é¢‘å’Œå­—å¹•æ–‡ä»¶
        logger.info("æ£€æŸ¥æœ¬åœ°æ˜¯å¦å·²æœ‰è§†é¢‘å’Œå­—å¹•æ–‡ä»¶...")
        existing_video_path = None
        existing_subtitle_path = None

        # æŸ¥æ‰¾æœ¬åœ°è§†é¢‘æ–‡ä»¶ï¼ˆåŒ¹é…æ ‡é¢˜ï¼‰
        video_extensions = ['.mp4', '.mkv', '.webm', '.flv', '.avi']
        for ext in video_extensions:
            potential_video = os.path.join(
                self.output_dir, f"{video_title}{ext}")
            if os.path.exists(potential_video) and os.path.getsize(potential_video) > 0:
                existing_video_path = potential_video
                logger.info(
                    f"âœ… æ‰¾åˆ°æœ¬åœ°è§†é¢‘æ–‡ä»¶: {os.path.basename(existing_video_path)}")
                break

        # å¦‚æœç²¾ç¡®åŒ¹é…æ²¡æ‰¾åˆ°ï¼Œå°è¯•æ¨¡ç³ŠåŒ¹é…
        if not existing_video_path:
            title_clean = video_title.replace(' ', '_').replace('/', '_')
            title_lower = video_title.lower()
            title_clean_lower = title_clean.lower()
            # æå–æ ‡é¢˜ä¸­çš„å…³é”®è¯ï¼ˆé•¿åº¦>2çš„å•è¯ï¼‰
            title_words = [w for w in re.split(
                r'[\s_\-]+', title_lower) if len(w) > 2]

            for f in os.listdir(self.output_dir):
                if f.endswith(tuple(video_extensions)) and not f.endswith(('.srt', '.vtt')):
                    f_lower = f.lower()
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«å®Œæ•´æ ‡é¢˜æˆ–æ¸…ç†åçš„æ ‡é¢˜
                    if title_lower in f_lower or title_clean_lower in f_lower:
                        potential_path = os.path.join(self.output_dir, f)
                        if os.path.getsize(potential_path) > 0:
                            existing_video_path = potential_path
                            logger.info(
                                f"âœ… æ‰¾åˆ°æœ¬åœ°è§†é¢‘æ–‡ä»¶ï¼ˆæ¨¡ç³ŠåŒ¹é…ï¼‰: {os.path.basename(existing_video_path)}")
                            break
                    # æˆ–è€…æ£€æŸ¥æ˜¯å¦åŒ…å«æ ‡é¢˜ä¸­çš„å¤šä¸ªå…³é”®è¯ï¼ˆè‡³å°‘2ä¸ªï¼‰
                    elif len(title_words) >= 2:
                        matched_words = sum(
                            1 for word in title_words if word in f_lower)
                        if matched_words >= 2:  # è‡³å°‘åŒ¹é…2ä¸ªå…³é”®è¯
                            potential_path = os.path.join(self.output_dir, f)
                            if os.path.getsize(potential_path) > 0:
                                existing_video_path = potential_path
                                logger.info(
                                    f"âœ… æ‰¾åˆ°æœ¬åœ°è§†é¢‘æ–‡ä»¶ï¼ˆå…³é”®è¯åŒ¹é…ï¼Œ{matched_words}/{len(title_words)}ï¼‰: {os.path.basename(existing_video_path)}")
                                break

        # æŸ¥æ‰¾æœ¬åœ°å­—å¹•æ–‡ä»¶ï¼ˆå®¹å¿ B ç«™/YouTube æ‰©å±•å‘½åï¼Œä¾‹å¦‚ *.NA.ai-zh.srtï¼‰
        existing_subtitle_path = self._find_local_subtitle_file(video_title)
        if existing_subtitle_path:
            logger.info(
                f"âœ… æ‰¾åˆ°æœ¬åœ°å­—å¹•æ–‡ä»¶: {os.path.basename(existing_subtitle_path)}")

        # æ£€æŸ¥å¯ç”¨å­—å¹•
        subtitle_lang = None
        try:
            sub_cmd = self._build_ytdlp_command([
                '--list-subs',
                '--skip-download',
                url
            ])
            sub_output = subprocess.run(
                sub_cmd, capture_output=True, text=True, check=True
            )
            available_subs = sub_output.stdout

            # æŸ¥æ‰¾è‹±æ–‡æˆ–ä¸­æ–‡å­—å¹•ï¼ˆä¼˜å…ˆä¸­æ–‡ï¼‰
            # æ£€æŸ¥è‡ªåŠ¨ç”Ÿæˆçš„å­—å¹•ï¼ˆé€šå¸¸ç”¨en,zhç­‰ç®€å†™ï¼‰
            # æ£€æŸ¥æ‰‹åŠ¨å­—å¹•ï¼ˆé€šå¸¸ç”¨en-US,zh-CNç­‰ï¼‰
            # æ£€æŸ¥Bilibili AIå­—å¹•ï¼ˆai-zh, ai-enç­‰ï¼‰
            # yt-dlp --list-subs è¾“å‡ºæ ¼å¼ï¼šLanguage    Formats (å¦‚ "ai-zh    srt")
            if re.search(r'\bai-zh\b', available_subs, re.IGNORECASE):
                subtitle_lang = 'ai-zh'
                logger.info("æ‰¾åˆ°ç®€ä½“ä¸­æ–‡å­—å¹• ai-zhï¼ˆä¼˜å…ˆä½¿ç”¨ï¼‰")
            elif re.search(r'\b(zh-cn|zh_CN|chinese)\b', available_subs, re.IGNORECASE):
                subtitle_lang = 'zh-cn'
                logger.info("æ‰¾åˆ°ç®€ä½“ä¸­æ–‡å­—å¹•ï¼ˆä¼˜å…ˆä½¿ç”¨ï¼‰")
            elif re.search(r'\b(zh-tw|zh_TW)\b', available_subs, re.IGNORECASE):
                subtitle_lang = 'zh-tw'
                logger.info("æ‰¾åˆ°ç¹ä½“ä¸­æ–‡å­—å¹•ï¼ˆä¼˜å…ˆä½¿ç”¨ï¼‰")
            elif re.search(r'\bzh\b', available_subs, re.IGNORECASE):
                subtitle_lang = 'zh'
                logger.info("æ‰¾åˆ°ä¸­æ–‡å­—å¹•ï¼ˆä¼˜å…ˆä½¿ç”¨ï¼‰")
            elif re.search(r'\bai-en\b', available_subs, re.IGNORECASE):
                subtitle_lang = 'ai-en'
                logger.info("æ‰¾åˆ°è‹±æ–‡å­—å¹• ai-en")
            elif re.search(r'\b(en|english)\b', available_subs, re.IGNORECASE):
                subtitle_lang = 'en'
                logger.info("æ‰¾åˆ°è‹±æ–‡å­—å¹•")
            else:
                logger.warning("æœªæ‰¾åˆ°ä¸­æ–‡æˆ–è‹±æ–‡å­—å¹•ï¼Œå°†å°è¯•ä¸‹è½½æ‰€æœ‰å¯ç”¨å­—å¹•")
                subtitle_lang = 'all'  # ä¸‹è½½æ‰€æœ‰å­—å¹•ï¼Œåç»­é€‰æ‹©
        except Exception as e:
            logger.warning(f"æ£€æŸ¥å­—å¹•å¤±è´¥: {e}ï¼Œå°†å°è¯•ä¸‹è½½æ‰€æœ‰å­—å¹•")
            subtitle_lang = 'all'

        # å¦‚æœå·²æœ‰æœ¬åœ°è§†é¢‘ï¼Œè·³è¿‡ä¸‹è½½
        if existing_video_path:
            logger.info("â­ï¸  è·³è¿‡è§†é¢‘ä¸‹è½½ï¼Œä½¿ç”¨æœ¬åœ°æ–‡ä»¶")
            video_path = existing_video_path
        else:
            # ä¸‹è½½è§†é¢‘ï¼ˆæœ€é«˜ç”»è´¨ï¼Œä¸ä¸‹è½½éŸ³é¢‘ï¼Œå› ä¸ºåªç”¨äºæˆªå›¾ï¼‰
            logger.info("æ­£åœ¨ä¸‹è½½è§†é¢‘ï¼ˆæœ€é«˜ç”»è´¨ï¼Œæ— éŸ³é¢‘ï¼Œä»…ç”¨äºæˆªå›¾ï¼‰...")
            # åªä¸‹è½½è§†é¢‘æµï¼ˆæœ€é«˜ç”»è´¨ï¼‰ï¼Œä¸ä¸‹è½½éŸ³é¢‘
            video_cmd = self._build_ytdlp_command([
                # ä¼˜å…ˆmp4ï¼Œå…¶æ¬¡720p+ï¼Œæœ€åä»»ä½•æœ€é«˜ç”»è´¨è§†é¢‘
                '-f', 'bestvideo[ext=mp4]/bestvideo[height>=720]/bestvideo',
                '--no-write-subs',  # ä¸ä¸‹è½½å­—å¹•ï¼ˆæˆ‘ä»¬ä¼šå•ç‹¬ä¸‹è½½ï¼‰
                '--no-playlist',  # ä¸ä¸‹è½½æ’­æ”¾åˆ—è¡¨
                '-o', video_template,
                url
            ])

            try:
                result = subprocess.run(
                    video_cmd, check=True, capture_output=True, text=True)
                # ç­‰å¾…æ–‡ä»¶å†™å…¥å®Œæˆ
                import time
                time.sleep(1)
            except subprocess.CalledProcessError as e:
                # å¦‚æœåªä¸‹è½½è§†é¢‘å¤±è´¥ï¼Œå°è¯•ä¸‹è½½è§†é¢‘+æœ€ä½éŸ³é¢‘
                logger.warning(
                    f"åªä¸‹è½½è§†é¢‘å¤±è´¥: {e.stderr if hasattr(e, 'stderr') and e.stderr else str(e)}")
                logger.info("å°è¯•ä¸‹è½½è§†é¢‘+æœ€ä½éŸ³é¢‘...")
                video_cmd_fallback = self._build_ytdlp_command([
                    '-f', 'bestvideo[ext=mp4]+worstaudio[ext=m4a]/bestvideo+worstaudio',
                    '--no-write-subs',
                    '--no-playlist',
                    '-o', video_template,
                    url
                ])
                try:
                    subprocess.run(video_cmd_fallback, check=True,
                                   capture_output=True, text=True)
                    import time
                    time.sleep(1)
                except Exception as e2:
                    logger.error(f"è§†é¢‘ä¸‹è½½å¤±è´¥: {e2}")
                    raise

            # æŸ¥æ‰¾ä¸‹è½½çš„è§†é¢‘æ–‡ä»¶ï¼ˆä¼˜å…ˆåŒ¹é…å½“å‰è§†é¢‘æ ‡é¢˜ï¼‰
            video_path = None
            title_clean = video_title.replace(' ', '_').replace('/', '_')

            # 1. ä¼˜å…ˆç²¾ç¡®åŒ¹é…ï¼šæ ‡é¢˜+æ‰©å±•å
            for ext in ['.mp4', '.mkv', '.webm', '.flv', '.avi']:
                potential_video = os.path.join(
                    self.output_dir, f"{video_title}{ext}")
                if os.path.exists(potential_video) and os.path.getsize(potential_video) > 0:
                    video_path = potential_video
                    logger.info(
                        f"âœ… æ‰¾åˆ°ä¸‹è½½çš„è§†é¢‘æ–‡ä»¶ï¼ˆç²¾ç¡®åŒ¹é…ï¼‰: {os.path.basename(video_path)}")
                    break

            # 2. å¦‚æœç²¾ç¡®åŒ¹é…æ²¡æ‰¾åˆ°ï¼Œå°è¯•æ¨¡ç³ŠåŒ¹é…å½“å‰è§†é¢‘æ ‡é¢˜
            if not video_path:
                matching_files = []
                for f in os.listdir(self.output_dir):
                    if f.endswith(('.mp4', '.mkv', '.webm', '.flv', '.avi')) and not f.endswith(('.srt', '.vtt')):
                        f_lower = f.lower()
                        title_lower = video_title.lower()
                        title_clean_lower = title_clean.lower()
                        # æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦åŒ…å«è§†é¢‘æ ‡é¢˜
                        if (title_lower in f_lower or title_clean_lower in f_lower or
                                any(part for part in title_lower.split() if len(part) > 3 and part in f_lower)):
                            matching_files.append(f)

                if matching_files:
                    # é€‰æ‹©åŒ¹é…æ–‡ä»¶ä¸­æœ€å¤§çš„ï¼ˆé€šå¸¸æ˜¯åˆšä¸‹è½½çš„ï¼‰
                    matching_files_with_size = [(f, os.path.getsize(os.path.join(self.output_dir, f)))
                                                for f in matching_files]
                    matching_files_with_size.sort(
                        key=lambda x: x[1], reverse=True)
                    video_path = os.path.join(
                        self.output_dir, matching_files_with_size[0][0])
                    logger.info(
                        f"âœ… æ‰¾åˆ°ä¸‹è½½çš„è§†é¢‘æ–‡ä»¶ï¼ˆæ¨¡ç³ŠåŒ¹é…ï¼‰: {os.path.basename(video_path)}")

            # 3. å¦‚æœè¿˜æ˜¯æ²¡æ‰¾åˆ°ï¼Œå°è¯•æ‰¾æœ€è¿‘ä¿®æ”¹çš„æ–‡ä»¶ï¼ˆå¯èƒ½æ˜¯åˆšä¸‹è½½çš„ï¼‰
            if not video_path:
                video_files = []
                for f in os.listdir(self.output_dir):
                    if f.endswith(('.mp4', '.mkv', '.webm', '.flv', '.avi')) and not f.endswith(('.srt', '.vtt')):
                        video_files.append(f)

                if video_files:
                    # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œé€‰æ‹©æœ€æ–°çš„æ–‡ä»¶
                    video_files_with_time = []
                    for f in video_files:
                        file_path = os.path.join(self.output_dir, f)
                        mtime = os.path.getmtime(file_path)
                        video_files_with_time.append(
                            (f, mtime, os.path.getsize(file_path)))

                    video_files_with_time.sort(
                        key=lambda x: x[1], reverse=True)  # æŒ‰ä¿®æ”¹æ—¶é—´é™åº
                    video_path = os.path.join(
                        self.output_dir, video_files_with_time[0][0])
                    logger.warning(
                        f"âš ï¸  æ— æ³•ç²¾ç¡®åŒ¹é…è§†é¢‘æ ‡é¢˜ï¼Œä½¿ç”¨æœ€è¿‘ä¿®æ”¹çš„æ–‡ä»¶: {os.path.basename(video_path)}")
                    logger.warning(f"   è¯·ç¡®è®¤è¿™æ˜¯æ­£ç¡®çš„è§†é¢‘æ–‡ä»¶ï¼")

            if not video_path:
                raise FileNotFoundError(f"æœªæ‰¾åˆ°ä¸‹è½½çš„è§†é¢‘æ–‡ä»¶ï¼ˆæ ‡é¢˜: {video_title}ï¼‰")

            file_size = os.path.getsize(video_path) / 1024 / 1024
            logger.info(
                f"è§†é¢‘æ–‡ä»¶: {os.path.basename(video_path)} ({file_size:.2f} MB)")

        # å¦‚æœå·²æœ‰æœ¬åœ°å­—å¹•ï¼Œè·³è¿‡ä¸‹è½½
        if existing_subtitle_path:
            logger.info("â­ï¸  è·³è¿‡å­—å¹•ä¸‹è½½ï¼Œä½¿ç”¨æœ¬åœ°æ–‡ä»¶")
            subtitle_path = existing_subtitle_path
        else:
            # ä¸‹è½½å­—å¹•ï¼ˆå¦‚æœå¯ç”¨ï¼Œåªä¸‹è½½srtæ ¼å¼ï¼‰
            subtitle_path = None
            if subtitle_lang and subtitle_lang != 'all':
                logger.info(f"æ­£åœ¨ä¸‹è½½å­—å¹• ({subtitle_lang})ï¼Œä»…SRTæ ¼å¼...")
                subtitle_cmd = self._build_ytdlp_command([
                    '--write-subs',
                    '--write-auto-subs',  # ä¹Ÿä¸‹è½½è‡ªåŠ¨ç”Ÿæˆçš„å­—å¹•
                    '--sub-langs', subtitle_lang,
                    '--sub-format', 'srt',  # åªä¸‹è½½srtæ ¼å¼
                    '--skip-download',
                    '-o', subtitle_template,
                    url
                ])

                try:
                    subprocess.run(subtitle_cmd, check=True,
                                   capture_output=True)
                except Exception as e:
                    logger.warning(f"å­—å¹•ä¸‹è½½å¤±è´¥: {e}")

            # å¦‚æœsubtitle_langæ˜¯'all'ï¼Œå°è¯•ä¸‹è½½æ‰€æœ‰å­—å¹•ï¼ˆä»…srtæ ¼å¼ï¼‰
            if subtitle_lang == 'all':
                logger.info("å°è¯•ä¸‹è½½æ‰€æœ‰å¯ç”¨å­—å¹•ï¼ˆä»…SRTæ ¼å¼ï¼‰...")
                try:
                    subtitle_cmd = self._build_ytdlp_command([
                        '--write-subs',
                        '--write-auto-subs',
                        '--sub-format', 'srt',  # åªä¸‹è½½srtæ ¼å¼
                        '--skip-download',
                        '-o', subtitle_template,
                        url
                    ])
                    subprocess.run(subtitle_cmd, check=True,
                                   capture_output=True)
                except Exception as e:
                    logger.warning(f"ä¸‹è½½æ‰€æœ‰å­—å¹•å¤±è´¥: {e}")

            # æŸ¥æ‰¾å­—å¹•æ–‡ä»¶ï¼ˆä¼˜å…ˆä¸­æ–‡ï¼Œå…¶æ¬¡è‹±æ–‡ï¼Œä»…srtæ ¼å¼ï¼‰
            if subtitle_lang:
                subtitle_path = self._find_local_subtitle_file(video_title)
                if subtitle_path:
                    logger.info(
                        f"é€‰æ‹©å­—å¹•æ–‡ä»¶: {os.path.basename(subtitle_path)}")

        return {
            'video': video_path,
            'subtitle': subtitle_path,
            'title': video_title
        }

    @staticmethod
    def _normalize_for_match(text: str) -> str:
        """å°†æ ‡é¢˜/æ–‡ä»¶åæ ‡å‡†åŒ–ï¼Œä¾¿äºæ¨¡ç³ŠåŒ¹é…"""
        text = text.lower()
        text = re.sub(r'\.na', '.', text)  # Bç«™å­—å¹•ä¼šå‡ºç° .NA
        return re.sub(r'[\s_\-\.]+', '', text)

    @staticmethod
    def _subtitle_lang_priority(filename: str) -> int:
        """å­—å¹•è¯­è¨€ä¼˜å…ˆçº§ï¼šai-zh > zh > ai-en > en > other"""
        name = filename.lower()
        if 'ai-zh' in name:
            return 5
        if any(tag in name for tag in ['zh-cn', 'zh_tw', 'zh', 'chinese', 'ä¸­æ–‡', 'cn']):
            return 4
        if 'ai-en' in name:
            return 3
        if any(tag in name for tag in ['en', 'english', 'è‹±æ–‡']):
            return 2
        return 1

    def _find_local_subtitle_file(self, video_title: str) -> str:
        """
        åœ¨è¾“å‡ºç›®å½•ä¸­æŸ¥æ‰¾æœ€åŒ¹é…çš„è§†é¢‘å­—å¹•æ–‡ä»¶ï¼Œå®¹å¿ B ç«™çš„ .NA/è¯­è¨€åç¼€
        """
        normalized_title_full = video_title.lower()
        normalized_title_simple = self._normalize_for_match(video_title)
        title_words = [w for w in re.split(
            r'[\s_\-]+', normalized_title_full) if len(w) > 2]

        best_candidate = None
        best_score = -1

        for filename in os.listdir(self.output_dir):
            if not filename.lower().endswith('.srt'):
                continue
            file_path = os.path.join(self.output_dir, filename)
            if os.path.getsize(file_path) <= 0:
                continue

            fname_lower = filename.lower()
            fname_simple = self._normalize_for_match(
                os.path.splitext(fname_lower)[0])

            match_score = 0
            if normalized_title_full in fname_lower or normalized_title_simple in fname_simple:
                match_score = 2
            else:
                matched_words = sum(
                    1 for word in title_words if word and word in fname_lower)
                if matched_words >= max(1, len(title_words) // 2):
                    match_score = 1

            if match_score == 0:
                continue

            lang_score = self._subtitle_lang_priority(fname_lower)
            total_score = lang_score * 10 + match_score

            if total_score > best_score:
                best_score = total_score
                best_candidate = file_path
            elif total_score == best_score and best_candidate:
                if os.path.getmtime(file_path) > os.path.getmtime(best_candidate):
                    best_candidate = file_path

        return best_candidate


class TimeRangeExtractor:
    """æ—¶é—´æ®µæå–å™¨ - åœ¨æŒ‡å®šæ—¶é—´æ®µå†…æå–è§†é¢‘å¸§"""

    @staticmethod
    def time_str_to_seconds(time_str: str) -> float:
        """
        å°†æ—¶é—´å­—ç¬¦ä¸²è½¬æ¢ä¸ºç§’æ•°
        æ”¯æŒæ ¼å¼: HH:MM:SS,mmm æˆ– HH:MM:SS.mmm
        """
        # æ›¿æ¢é€—å·ä¸ºç‚¹
        time_str = time_str.replace(',', '.')

        # è§£ææ—¶é—´
        parts = time_str.split(':')
        hours = int(parts[0])
        minutes = int(parts[1])
        seconds = float(parts[2])

        return hours * 3600 + minutes * 60 + seconds

    @staticmethod
    def seconds_to_time_str(seconds: float) -> str:
        """å°†ç§’æ•°è½¬æ¢ä¸º HH:MM:SS æ ¼å¼"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        return f"{hours:02d}:{minutes:02d}:{secs:02d}"

    @staticmethod
    def extract_frames_in_range(video_path: str, start_time: float, end_time: float,
                                output_dir: str, interval: float = 2.0,
                                image_format: str = 'jpg', quality: int = 95,
                                skip_similar: bool = True,
                                similarity_threshold: float = 0.95) -> List[str]:
        """
        åœ¨æŒ‡å®šæ—¶é—´æ®µå†…æå–è§†é¢‘å¸§

        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            start_time: å¼€å§‹æ—¶é—´ï¼ˆç§’ï¼‰
            end_time: ç»“æŸæ—¶é—´ï¼ˆç§’ï¼‰
            output_dir: è¾“å‡ºç›®å½•
            interval: æå–é—´éš”ï¼ˆç§’ï¼‰
            image_format: å›¾ç‰‡æ ¼å¼
            quality: å›¾ç‰‡è´¨é‡

        Returns:
            æå–çš„å›¾ç‰‡æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        os.makedirs(output_dir, exist_ok=True)

        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise ValueError(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")

        fps = cap.get(cv2.CAP_PROP_FPS)
        start_frame = int(start_time * fps)
        end_frame = int(end_time * fps)
        frame_interval = max(1, int(fps * interval))

        # è®¾ç½®å›¾ç‰‡ç¼–ç å‚æ•°
        if image_format.lower() == 'jpg' or image_format.lower() == 'jpeg':
            encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), quality]
            ext = 'jpg'
        elif image_format.lower() == 'png':
            encode_param = [int(cv2.IMWRITE_PNG_COMPRESSION), 3]
            ext = 'png'
        else:
            encode_param = []
            ext = image_format.lower()

        extracted_files = []
        frame_count = 0
        extracted_count = 0
        skipped_similar = 0
        last_frame = None

        # è·³è½¬åˆ°å¼€å§‹ä½ç½®
        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)

        while frame_count <= (end_frame - start_frame):
            ret, frame = cap.read()

            if not ret:
                break

            # æ£€æŸ¥æ˜¯å¦è¶…å‡ºç»“æŸæ—¶é—´
            current_time = start_time + (frame_count / fps)
            if current_time > end_time:
                break

            # æŒ‰é—´éš”æå–
            if frame_count % frame_interval == 0:
                is_similar = False
                if skip_similar and last_frame is not None:
                    similarity = TimeRangeExtractor._calculate_similarity(
                        last_frame, frame)
                    if similarity >= similarity_threshold:
                        is_similar = True
                        skipped_similar += 1

                if not is_similar:
                    time_str = TimeRangeExtractor.seconds_to_time_str(
                        current_time)
                    filename = f"frame_{extracted_count:03d}_{time_str.replace(':', '')}.{ext}"
                    filepath = os.path.join(output_dir, filename)

                    if encode_param:
                        cv2.imwrite(filepath, frame, encode_param)
                    else:
                        cv2.imwrite(filepath, frame)

                    extracted_files.append(filepath)
                    extracted_count += 1
                    if skip_similar:
                        last_frame = frame.copy()

            frame_count += 1

        cap.release()

        if skip_similar and skipped_similar > 0:
            logger.info(f"    è·³è¿‡ç›¸ä¼¼å¸§: {skipped_similar}")

        return extracted_files

    @staticmethod
    def _calculate_similarity(frame1, frame2) -> float:
        """è®¡ç®—ä¸¤å¸§ä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼ˆä½¿ç”¨ç¼©æ”¾åçš„MSEï¼‰"""
        gray1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)
        gray2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)

        small_size = (128, 72)
        gray1_small = cv2.resize(gray1, small_size)
        gray2_small = cv2.resize(gray2, small_size)

        mse = np.mean(
            (gray1_small.astype(float) - gray2_small.astype(float)) ** 2)
        max_mse = 255.0 ** 2
        similarity = 1.0 - (mse / max_mse)
        return similarity


class VideoSummaryApp:
    """è§†é¢‘æ€»ç»“åº”ç”¨ä¸»ç±»"""

    def __init__(self, output_dir: str = "output", test_mode: bool = False, cookies_file: str = None):
        """
        åˆå§‹åŒ–åº”ç”¨

        Args:
            output_dir: è¾“å‡ºç›®å½•
            test_mode: æ˜¯å¦å¯ç”¨æµ‹è¯•æ¨¡å¼ï¼ˆä¸è°ƒç”¨LLMï¼Œä»…è¾“å‡ºPromptï¼‰
            cookies_file: Cookies æ–‡ä»¶è·¯å¾„ï¼ˆç”¨äº Bilibili ç­‰éœ€è¦ç™»å½•çš„ç½‘ç«™ï¼‰
        """
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)
        self.downloader = VideoDownloader(
            os.path.join(output_dir, "downloads"), cookies_file=cookies_file)
        self.time_extractor = TimeRangeExtractor()
        self.test_mode = test_mode

    def process_video(self, url: str,
                      frame_extraction_interval: float = 2.0,
                      skip_similar_frames: bool = True) -> str:
        """
        å¤„ç†è§†é¢‘ï¼šä¸‹è½½ã€è§£æã€æ€»ç»“ã€æå–å¸§ã€ç”Ÿæˆmarkdown

        Args:
            url: è§†é¢‘é“¾æ¥
            frame_extraction_interval: å¸§æå–é—´éš”ï¼ˆç§’ï¼‰
            skip_similar_frames: æ˜¯å¦è·³è¿‡ç›¸ä¼¼å¸§

        Returns:
            ç”Ÿæˆçš„markdownæ–‡ä»¶è·¯å¾„
        """
        logger.info("=" * 60)
        logger.info("å¼€å§‹å¤„ç†è§†é¢‘")
        logger.info("=" * 60)

        # 1. ä¸‹è½½è§†é¢‘å’Œå­—å¹•
        logger.info("\n[æ­¥éª¤ 1/5] ä¸‹è½½è§†é¢‘å’Œå­—å¹•...")
        download_result = self.downloader.download(url)
        video_path = download_result['video']
        subtitle_path = download_result['subtitle']
        video_title = download_result['title']

        if not subtitle_path:
            logger.error("æœªæ‰¾åˆ°å­—å¹•æ–‡ä»¶ï¼Œæ— æ³•ç»§ç»­å¤„ç†")
            raise ValueError("éœ€è¦å­—å¹•æ–‡ä»¶æ‰èƒ½ç”Ÿæˆæ€»ç»“")

        # 2. è§£æå­—å¹•
        logger.info("\n[æ­¥éª¤ 2/5] è§£æå­—å¹•...")
        with open(subtitle_path, 'r', encoding='utf-8') as f:
            subtitle_content = f.read()

        subtitle_data, consolidated_text = parse_subtitles(subtitle_content)
        logger.info(f"è§£æå®Œæˆ: å…± {len(subtitle_data)} æ¡å­—å¹•")

        # ä¿å­˜æ–‡ç¨¿åˆ°ä¸´æ—¶æ–‡ä»¶
        temp_text_file = os.path.join(
            self.output_dir, f"{video_title}_transcript.txt")
        with open(temp_text_file, 'w', encoding='utf-8') as f:
            f.write(consolidated_text)
        logger.info(f"æ–‡ç¨¿å·²ä¿å­˜: {temp_text_file}")

        # 3 & 4. AIæ€»ç»“ ä¸ å…³é”®å¸§æå–ï¼ˆå¹¶è¡Œï¼‰
        logger.info("\n[æ­¥éª¤ 3/5] ç”ŸæˆAIæ€»ç»“...")
        logger.info("\n[æ­¥éª¤ 4/5] æå–å…³é”®å¸§ï¼ˆä¸æ­¥éª¤ 3 å¹¶è¡Œæ‰§è¡Œï¼‰...")

        # æ£€æµ‹è¯­è¨€å¹¶åˆ‡åˆ†æ–‡æœ¬ï¼ˆæŒ‰è¯/å­—æ•°é‡ï¼‰
        language = detect_language(consolidated_text)
        CHUNK_SIZE = 2000 if language == "Chinese" else 1700
        OVERLAP = 150 if language == "Chinese" else 120

        chunks = self._split_subtitles_into_chunks(
            subtitle_data, CHUNK_SIZE, OVERLAP)
        chunk_texts = [chunk['text'] for chunk in chunks]

        logger.info(f"æ–‡æœ¬å·²åˆ‡åˆ†ä¸º {len(chunks)} ä¸ªç‰‡æ®µ")
        for idx, chunk in enumerate(chunks, start=1):
            word_count = self._count_words(chunk['text'])
            logger.info(f"  - ç‰‡æ®µ {idx}/{len(chunks)} è¯æ•°: {word_count}")

        frames_dir = os.path.join(self.output_dir, f"{video_title}_frames")
        os.makedirs(frames_dir, exist_ok=True)

        with ThreadPoolExecutor(max_workers=2) as executor:
            summary_future = executor.submit(
                self._generate_summary_with_chunks,
                temp_text_file, chunk_texts, video_title
            )
            frames_future = executor.submit(
                self._extract_frames_for_chunks,
                video_path, chunks, frames_dir,
                frame_extraction_interval, skip_similar_frames
            )
            summary_path = summary_future.result()
            chunk_frames = frames_future.result()

        # 5. ç”Ÿæˆæœ€ç»ˆmarkdown
        logger.info("\n[æ­¥éª¤ 5/5] ç”Ÿæˆæœ€ç»ˆmarkdownæ–‡æ¡£...")
        final_md_path = self._generate_final_markdown(
            summary_path, chunk_texts, chunk_frames, video_title, video_path
        )

        logger.info("=" * 60)
        logger.info("âœ… å¤„ç†å®Œæˆï¼")
        logger.info(f"ğŸ“„ æœ€ç»ˆæ–‡æ¡£: {final_md_path}")
        logger.info("=" * 60)

        return final_md_path

    def _split_subtitles_into_chunks(self, subtitle_data: SubtitleData,
                                     chunk_size: int, overlap: int) -> List[Dict[str, Any]]:
        """
        åŸºäºå­—å¹•æ•°æ®æŒ‰è¯æ•°åˆ‡åˆ†ï¼Œå¹¶è¿”å›æ¯æ®µçš„æ–‡æœ¬å’Œæ—¶é—´èŒƒå›´
        """
        if not subtitle_data:
            return []

        token_pattern = re.compile(r'[\u4e00-\u9fff]|[a-zA-Z0-9]+')
        entry_token_counts = []
        for entry in subtitle_data:
            tokens = token_pattern.findall(entry['text'])
            entry_token_counts.append(max(1, len(tokens)))

        cumulative = [0]
        for count in entry_token_counts:
            cumulative.append(cumulative[-1] + count)

        chunks = []
        start_idx = 0
        total_entries = len(subtitle_data)

        while start_idx < total_entries:
            start_tokens = cumulative[start_idx]
            target_tokens = start_tokens + chunk_size
            end_idx = bisect_left(cumulative, target_tokens, lo=start_idx + 1)
            if end_idx <= start_idx:
                end_idx = start_idx + 1
            if end_idx > total_entries:
                end_idx = total_entries

            chunk_entries = subtitle_data[start_idx:end_idx]
            chunk_text_parts = [
                entry['text'].strip() for entry in chunk_entries if entry['text'].strip()]
            chunk_text = "\n".join(chunk_text_parts).strip()

            start_time = TimeRangeExtractor.time_str_to_seconds(
                chunk_entries[0]['start'])
            end_time = TimeRangeExtractor.time_str_to_seconds(
                chunk_entries[-1]['end'])

            chunks.append({
                'text': chunk_text,
                'start_time': start_time,
                'end_time': end_time,
                'start_index': start_idx,
                'end_index': end_idx
            })

            if end_idx >= total_entries:
                break

            next_tokens = max(0, cumulative[end_idx] - overlap)
            start_idx = bisect_left(cumulative, next_tokens)
            if start_idx >= total_entries:
                break
            # ç¡®ä¿è‡³å°‘å‘å‰æ¨è¿›
            if start_idx == end_idx:
                start_idx += 1

        return chunks

    def _generate_summary_with_chunks(self, text_file: str, chunks: List[str],
                                      video_title: str) -> str:
        """
        ç”Ÿæˆæ€»ç»“å¹¶è¿”å›æ€»ç»“æ–‡ä»¶è·¯å¾„
        è¿™é‡Œéœ€è¦è°ƒç”¨Summary.pyçš„åŠŸèƒ½ï¼Œä½†éœ€è¦è·å–æ¯ä¸ªchunkçš„æ€»ç»“
        """
        # ç”Ÿæˆæ¯ä¸ªchunkçš„æ€»ç»“
        summaries = []
        total_chunks = len(chunks)

        if self.test_mode:
            logger.info("ğŸ”§ æµ‹è¯•æ¨¡å¼å¼€å¯ï¼šä¸ä¼šè°ƒç”¨LLMï¼Œç›´æ¥è¾“å‡ºPromptå†…å®¹")
            for i, chunk in enumerate(chunks):
                current_idx = i + 1
                prompt = BASE_SYSTEM_PROMPT.format(
                    current=current_idx, total=total_chunks) + "\n\n" + chunk
                summaries.append(prompt)
        else:
            # æ£€æŸ¥API KEY
            api_key = os.environ.get("GEMINI_API_KEY", GEMINI_API_KEY)
            if not api_key or "YOUR_API_KEY" in api_key:
                raise ValueError("GEMINI_API_KEY æœªè®¾ç½®ï¼Œè¯·é…ç½®ç¯å¢ƒå˜é‡æˆ–åœ¨ä»£ç ä¸­å¡«å†™ã€‚")
            if genai is None:
                raise ImportError(
                    "æœªæ‰¾åˆ° google-genaiï¼Œè¯·å…ˆå®‰è£…: pip install google-genai")

            try:
                client = genai.Client(api_key=api_key, http_options={
                    'api_version': 'v1alpha'})
            except Exception as e:
                logger.error(f"åˆå§‹åŒ–APIå®¢æˆ·ç«¯å¤±è´¥: {e}")
                raise

            for i, chunk in enumerate(chunks):
                current_idx = i + 1

                logger.info(f"  æ€»ç»“ç‰‡æ®µ {current_idx}/{total_chunks}...")
                try:
                    summary = generate_chunk_summary(
                        client, chunk, current_idx, total_chunks, PRIMARY_MODEL
                    )
                    if summary:
                        summaries.append(summary)
                    else:
                        summaries.append(f"\n> [é”™è¯¯: ç¬¬ {current_idx} éƒ¨åˆ†æ€»ç»“ä¸ºç©º]\n")
                except Exception as e:
                    logger.warning(f"  ç‰‡æ®µ {current_idx} æ€»ç»“å¤±è´¥: {e}")
                    summaries.append(
                        f"\n> [é”™è¯¯: ç¬¬ {current_idx} éƒ¨åˆ†æ€»ç»“å¤±è´¥: {str(e)}]\n")

        # ä¿å­˜æ€»ç»“åˆ°æ–‡ä»¶
        summary_path = os.path.join(
            self.output_dir, f"{video_title}_summary_temp.md")
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write(f"# {video_title} å­¦ä¹ ç¬”è®°\n\n")
            f.write(f"> ç”± AI ç”Ÿæˆï¼Œå…± {len(chunks)} éƒ¨åˆ†\n\n")

            for i, summary in enumerate(summaries):
                f.write(f"\n## ç¬¬ {i+1} éƒ¨åˆ†\n\n")
                f.write(summary)
                f.write("\n\n---\n")

        return summary_path

    def _extract_frames_for_chunks(self, video_path: str,
                                   chunks: List[Dict[str, Any]],
                                   frames_dir: str,
                                   frame_extraction_interval: float,
                                   skip_similar_frames: bool) -> Dict[int, List[str]]:
        """
        ä¸ºæ¯ä¸ªç‰‡æ®µæå–å¸§ï¼Œè¿”å›ç‰‡æ®µç´¢å¼•åˆ°å¸§è·¯å¾„åˆ—è¡¨çš„æ˜ å°„
        """
        chunk_frames: Dict[int, List[str]] = {}

        for i, chunk in enumerate(chunks):
            start_time = chunk['start_time']
            end_time = chunk['end_time']
            time_str = f"{int(start_time//60):02d}m{int(start_time % 60):02d}s-{int(end_time//60):02d}m{int(end_time % 60):02d}s"
            chunk_frames_dir = os.path.join(
                frames_dir, f"chunk_{i+1:02d}_{time_str}")

            # æµ‹è¯•æ¨¡å¼ä¸‹å¦‚æœç›®å½•å·²å­˜åœ¨åˆ™ç›´æ¥å¤ç”¨ï¼Œé¿å…é‡æ–°æå–
            if self.test_mode and os.path.isdir(chunk_frames_dir):
                logger.info(
                    f"  ç‰‡æ®µ {i+1}/{len(chunks)}: {time_str} -> ä½¿ç”¨ç°æœ‰å¸§ç›®å½•ï¼Œè·³è¿‡æå–")
                existing_files = [
                    os.path.join(chunk_frames_dir, f)
                    for f in sorted(os.listdir(chunk_frames_dir))
                    if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))
                ]
                chunk_frames[i] = existing_files
                logger.info(f"    å¤ç”¨ {len(existing_files)} å¸§")
                continue

            logger.info(f"  ç‰‡æ®µ {i+1}/{len(chunks)}: {time_str} -> æå–å¸§...")
            frame_files = self.time_extractor.extract_frames_in_range(
                video_path, start_time, end_time,
                chunk_frames_dir,
                interval=frame_extraction_interval,
                skip_similar=skip_similar_frames
            )
            chunk_frames[i] = frame_files
            logger.info(f"    æå–äº† {len(frame_files)} å¸§")

        return chunk_frames

    def _generate_final_markdown(self, summary_path: str, chunks: List[str],
                                 chunk_frames: Dict[int, List[str]],
                                 video_title: str, video_path: str) -> str:
        """
        ç”Ÿæˆæœ€ç»ˆçš„markdownæ–‡æ¡£ï¼ŒåŒ…å«æ€»ç»“å’Œæˆªå›¾
        """
        # è¯»å–æ€»ç»“å†…å®¹
        with open(summary_path, 'r', encoding='utf-8') as f:
            summary_content = f.read()

        final_md_path = os.path.join(self.output_dir, f"{video_title}_æœ€ç»ˆæ€»ç»“.md")

        with open(final_md_path, 'w', encoding='utf-8') as f:
            # å†™å…¥æ–‡ä»¶å¤´
            f.write(f"# {video_title} è§†é¢‘æ€»ç»“\n\n")
            f.write(
                f"> ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write(f"> æºè§†é¢‘: {os.path.basename(video_path)}\n\n")
            f.write("---\n\n")

            # è§£ææ€»ç»“ï¼Œæ‰¾åˆ°æ¯ä¸ªéƒ¨åˆ†
            # ä½¿ç”¨æ›´çµæ´»çš„æ–¹å¼åˆ†å‰²å†…å®¹
            parts = re.split(r'\n## ç¬¬ (\d+) éƒ¨åˆ†\n', summary_content)

            # å¦‚æœåˆ†å‰²æˆåŠŸï¼Œpartsåº”è¯¥æ˜¯: [æ ‡é¢˜å’Œå¼€å¤´å†…å®¹, '1', ç¬¬ä¸€éƒ¨åˆ†å†…å®¹, '2', ç¬¬äºŒéƒ¨åˆ†å†…å®¹, ...]
            if len(parts) > 1:
                # å†™å…¥å¼€å¤´å†…å®¹ï¼ˆå¦‚æœæœ‰ï¼‰
                if parts[0].strip():
                    # è·³è¿‡æ–‡ä»¶å¤´ï¼ˆ# æ ‡é¢˜ å’Œ > æ³¨é‡Šï¼‰
                    header_end = parts[0].find('\n---\n')
                    if header_end > 0:
                        parts[0] = parts[0][header_end + 5:]
                    if parts[0].strip():
                        f.write(parts[0].strip())
                        f.write("\n\n---\n\n")

                # å¤„ç†æ¯ä¸ªéƒ¨åˆ†
                for i in range(1, len(parts), 2):
                    if i + 1 >= len(parts):
                        break

                    part_num = parts[i]
                    part_content = parts[i + 1]

                    try:
                        chunk_idx = int(part_num) - 1
                    except ValueError:
                        continue

                    # å†™å…¥éƒ¨åˆ†æ ‡é¢˜
                    f.write(f"\n## ç¬¬ {part_num} éƒ¨åˆ†\n\n")

                    # å…ˆå±•ç¤ºæˆªå›¾
                    if chunk_idx in chunk_frames and chunk_frames[chunk_idx]:
                        f.write("### ğŸ“¸ ç›¸å…³æˆªå›¾\n\n")
                        for frame_path in chunk_frames[chunk_idx]:
                            if os.path.exists(frame_path):
                                try:
                                    rel_path = os.path.relpath(
                                        frame_path, os.path.dirname(final_md_path))
                                    rel_path = self._format_md_path(rel_path)
                                    f.write(f"![æˆªå›¾]({rel_path})\n\n")
                                except ValueError:
                                    fallback_path = self._format_md_path(
                                        frame_path)
                                    f.write(
                                        f"![æˆªå›¾]({fallback_path})\n\n")

                    # å†å†™æ€»ç»“å†…å®¹ï¼ˆå»é™¤æœ«å°¾çš„---åˆ†éš”ç¬¦ï¼‰
                    part_content_clean = part_content.rstrip(
                        '\n').rstrip('---').rstrip('\n').strip()
                    f.write(part_content_clean)
                    f.write("\n\n---\n\n")
            else:
                # å¦‚æœæ— æ³•åˆ†å‰²ï¼Œç›´æ¥å†™å…¥æ•´ä¸ªå†…å®¹
                f.write(summary_content)

        return final_md_path

    @staticmethod
    def _count_words(text: str) -> int:
        """
        ç»Ÿè®¡ä¸­è‹±æ–‡è¯æ•°ï¼šä¸­æ–‡é€å­—è®¡æ•°ï¼Œè‹±æ–‡æŒ‰è¿ç»­å­—æ¯æ•°å­—è®¡æ•°
        """
        tokens = re.findall(r'[\u4e00-\u9fff]|[a-zA-Z0-9]+', text)
        return len(tokens)

    @staticmethod
    def _format_md_path(path: str) -> str:
        """
        å°†æ–‡ä»¶è·¯å¾„è§„èŒƒåŒ–ä¸º Markdown å¯ç”¨çš„ URLï¼Œå¤„ç†ç©ºæ ¼ç­‰ç‰¹æ®Šå­—ç¬¦
        """
        normalized = path.replace(os.sep, '/')
        return quote(normalized, safe="/:-_.()")


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(
        description='è§†é¢‘æ€»ç»“åº”ç”¨ - ä¸‹è½½è§†é¢‘ã€ç”ŸæˆAIæ€»ç»“å¹¶æå–å…³é”®å¸§',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python video_summary_app.py "https://www.youtube.com/watch?v=xxx"
  python video_summary_app.py "https://www.bilibili.com/video/xxx" -o my_output
  python video_summary_app.py "https://www.bilibili.com/video/xxx" -c cookies.txt
  python video_summary_app.py "https://youtube.com/watch?v=xxx" -i 3.0
  
å‚æ•°è¯´æ˜:
  -o: è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤: outputï¼‰
  -i: å¸§æå–é—´éš”ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤: 2.0
  -c: Cookies æ–‡ä»¶è·¯å¾„ï¼ˆç”¨äº Bilibili ç­‰éœ€è¦ç™»å½•çš„ç½‘ç«™ï¼‰
        """
    )

    parser.add_argument('url', help='è§†é¢‘é“¾æ¥ï¼ˆYouTube/Bilibiliç­‰ï¼‰')
    parser.add_argument('-o', '--output', default='output',
                        help='è¾“å‡ºç›®å½•ï¼Œé»˜è®¤: output')
    parser.add_argument('-i', '--interval', type=float, default=2.0,
                        help='å¸§æå–é—´éš”ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤: 2.0')
    parser.add_argument(
        '-t', '--test', action='store_true',
        help='æµ‹è¯•æ¨¡å¼ï¼šä¸è°ƒç”¨LLMï¼Œç›´æ¥æŠŠPromptå†™å…¥è¾“å‡ºï¼Œä¾¿äºæŸ¥çœ‹ä¸Šä¸‹æ–‡')
    parser.add_argument(
        '-c', '--cookies', type=str, default=None,
        help='Cookies æ–‡ä»¶è·¯å¾„ï¼ˆç”¨äº Bilibili ç­‰éœ€è¦ç™»å½•çš„ç½‘ç«™ï¼‰ï¼Œä¾‹å¦‚: --cookies cookies.txt')

    args = parser.parse_args()

    try:
        # éªŒè¯ cookies æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        cookies_file = args.cookies
        if cookies_file and not os.path.exists(cookies_file):
            logger.warning(f"è­¦å‘Š: Cookies æ–‡ä»¶ä¸å­˜åœ¨: {cookies_file}ï¼Œå°†ä¸ä½¿ç”¨ cookies")
            cookies_file = None
        elif cookies_file:
            logger.info(f"âœ… ä½¿ç”¨ Cookies æ–‡ä»¶: {cookies_file}")

        app = VideoSummaryApp(output_dir=args.output,
                              test_mode=args.test, cookies_file=cookies_file)
        result_path = app.process_video(
            args.url, frame_extraction_interval=args.interval)
        print(f"\nâœ… å®Œæˆï¼ç»“æœæ–‡ä»¶: {result_path}")
    except Exception as e:
        logger.error(f"å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()
