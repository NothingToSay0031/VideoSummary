#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è§†é¢‘æ€»ç»“åº”ç”¨ - å®Œæ•´çš„YouTube/Bilibiliè§†é¢‘æ€»ç»“å·¥å…·
è¾“å…¥è§†é¢‘é“¾æ¥ï¼Œä¸‹è½½è§†é¢‘å’Œå­—å¹•ï¼Œç”ŸæˆAIæ€»ç»“å¹¶é™„å¸¦æˆªå›¾
"""

import os
import sys
import re
import subprocess
import json
import logging
import time
from datetime import datetime
from bisect import bisect_left
from urllib.parse import quote
from typing import List, Dict, Any, Tuple, TextIO, Optional
from concurrent.futures import ThreadPoolExecutor
import cv2
import numpy as np

try:
    from google import genai
except ImportError:
    genai = None

INVALID_PATH_CHARS = set('<>:"/\\|?*')


def sanitize_filename(name: str) -> str:
    """
    å°†ä¸é€‚åˆä½œä¸ºæ–‡ä»¶/æ–‡ä»¶å¤¹åçš„å­—ç¬¦æ›¿æ¢ä¸ºä¸‹åˆ’çº¿
    """
    if not name:
        return "untitled"

    sanitized_chars = []
    for ch in name:
        if ch in INVALID_PATH_CHARS or ord(ch) < 32 or ch.isspace():
            sanitized_chars.append('_')
        else:
            sanitized_chars.append(ch)

    sanitized = ''.join(sanitized_chars)
    sanitized = re.sub(r'_+', '_', sanitized).strip('_')
    return sanitized or "untitled"


def chinese_char_ratio(text: str) -> float:
    """
    ç»Ÿè®¡æ–‡æœ¬ä¸­æ±‰å­—æ‰€å æ¯”ä¾‹ï¼ˆå¿½ç•¥ç©ºç™½å­—ç¬¦ï¼‰
    """
    if not text:
        return 0.0
    total_chars = len([ch for ch in text if not ch.isspace()])
    if total_chars == 0:
        return 0.0
    chinese_count = len(re.findall(r'[\u4e00-\u9fff]', text))
    return chinese_count / total_chars


# ==== LLM é…ç½®ï¼ˆå¯æ ¹æ®éœ€è¦ä¿®æ”¹ï¼‰====
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY", "")
PRIMARY_MODEL = "gemini-2.5-pro"
BASE_SYSTEM_PROMPT = """
## Background Information

You are an expert proficient in computer graphics, skilled at explaining complex technical concepts in a clear, structured manner to people who have a foundation in graphics but wish to delve deeper into the content of a video lecture.
You are reading through and summarizing a long technical graphics lecture transcript section by section. This is part ${current}$ of ${total}$ total parts.
My Goal: I am a **Game Engine Engineer and Rendering Engineer** with a **graphics background**. I aim to enrich my knowledge, gain in-depth mastery of graphics and game engine knowledge, and understand industry developments, thus seeking to **deeply study the content related to the lecture**.

## Task Requirements

1.  **Core Task:** Please summarize the content I provide below into **easily understandable and memorable notes**.
2.  **Formatting Requirements:**
    * Use a **clear hierarchical structure** (main headings, subheadings, bullet points).
    * For each topic, distill and **bold** the **core concepts** and **key terminology**.
    * If **formulas or important algorithms** are involved, please highlight them. Use standard **LaTeX format** to ensure the document can be rendered correctly.
    * The language style should be as **accessible as possible**, avoiding direct copying of obscure jargon from the original text.
    * **Strictly No Meta-talk:**
        * **Absolutely prohibited** to output any opening or closing remarks (e.g., "...Here are the notes prepared for you...").
        * **Absolutely prohibited** to include metadata titles like "Part ${current}$" or "Part X" in the body text.
        * **Start directly with the technical content title.**
3. è¾“å‡ºä¸­æ–‡ï¼
## Content to be Summarized
"""

# ==== å­—å¹•è§£æé€»è¾‘====
SubtitleEntry = Dict[str, str]
SubtitleData = List[SubtitleEntry]


# ==== å­—å¹•å»é‡é€»è¾‘====
def is_timecode(line: str) -> bool:
    """åˆ¤æ–­ä¸€è¡Œæ˜¯å¦æ˜¯æ—¶é—´è½´"""
    return "-->" in line and line[0].isdigit()


def parse_srt_robust(file_path: str) -> List[Dict[str, Any]]:
    """
    ç¨³å¥çš„ SRT è§£æå™¨ï¼šä¸ä¾èµ–ç©ºè¡Œï¼Œè€Œæ˜¯æ ¹æ®æ—¶é—´è½´ç‰¹å¾æ¥åˆ‡åˆ†ã€‚
    è§£å†³ 'åºå·å’Œæ—¶é—´è½´è¢«å½“æˆæ–‡æœ¬' çš„ Bugã€‚
    """
    with open(file_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()

    blocks = []
    current_block = {"seq": None, "time": None, "text_lines": []}

    i = 0
    while i < len(lines):
        line = lines[i].strip()

        # æ ¸å¿ƒé€»è¾‘ï¼šæ‰«æåˆ°æ—¶é—´è½´ï¼Œè¯´æ˜æŠ“åˆ°äº†ä¸€ä¸ªæ–°å—çš„"éª¨æ¶"
        if is_timecode(line):
            # 1. ä¿å­˜ä¸Šä¸€ä¸ªå—ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if current_block["time"]:
                blocks.append(current_block)

            # 2. å¼€å§‹æ–°å—
            # æ—¶é—´è½´çš„å‰ä¸€è¡Œé€šå¸¸æ˜¯åºå·ï¼Œå°è¯•è·å–
            seq = "0"
            if i > 0 and lines[i-1].strip().isdigit():
                seq = lines[i-1].strip()

            current_block = {
                "seq": seq,
                "time": line,
                "text_lines": []  # å‡†å¤‡æ¥æ”¶æ¥ä¸‹æ¥çš„æ–‡æœ¬
            }

        # å¦‚æœä¸æ˜¯æ—¶é—´è½´ï¼Œä¹Ÿä¸æ˜¯æ—¶é—´è½´å‰é¢çš„é‚£ä¸ªåºå·ï¼Œé‚£å°±æ˜¯æ–‡æœ¬å†…å®¹
        elif line and not line.isdigit():
            # é˜²æ­¢æŠŠä¸‹ä¸€è¡Œçš„åºå·è¯¯è¯»ä¸ºæ–‡æœ¬ï¼š
            # åªæœ‰å½“ä¸‹ä¸€è¡Œä¸æ˜¯æ—¶é—´è½´æ—¶ï¼Œå½“å‰è¡Œæ‰å¯èƒ½æ˜¯æ–‡æœ¬
            is_next_line_time = (
                i + 1 < len(lines) and is_timecode(lines[i+1].strip()))
            if not is_next_line_time:
                if current_block["time"]:  # ç¡®ä¿å·²ç»åœ¨ä¸€ä¸ªå—é‡Œäº†
                    current_block["text_lines"].append(line)

        i += 1

    # åˆ«å¿˜äº†ä¿å­˜æœ€åä¸€ä¸ªå—
    if current_block["time"]:
        blocks.append(current_block)

    return blocks


def get_longest_overlap(s1: str, s2: str) -> int:
    """è®¡ç®—é‡å é•¿åº¦é€»è¾‘"""
    if not s1 or not s2:
        return 0
    min_overlap = 4
    max_possible = min(len(s1), len(s2))
    for length in range(max_possible, min_overlap - 1, -1):
        if s1.endswith(s2[:length]):
            return length
    return 0


def remove_duplicates_from_srt(file_path: str) -> bool:
    """
    å»é™¤ SRT å­—å¹•æ–‡ä»¶ä¸­çš„é‡å¤å†…å®¹
    
    Args:
        file_path: å­—å¹•æ–‡ä»¶è·¯å¾„
        
    Returns:
        æ˜¯å¦æˆåŠŸå¤„ç†ï¼ˆTrueï¼‰æˆ–å¤±è´¥ï¼ˆFalseï¼‰
    """
    try:
        logger.info(f"ğŸ§¹ å¼€å§‹å¯¹å­—å¹•è¿›è¡Œå»é‡å¤„ç†: {os.path.basename(file_path)}")
        blocks = parse_srt_robust(file_path)

        if not blocks:
            logger.warning("  -> ç©ºæ–‡ä»¶æˆ–è§£æå¤±è´¥ï¼Œè·³è¿‡å»é‡")
            return False

        final_blocks = []
        prev_text = ""

        for block in blocks:
            # å°†å¤šè¡Œæ–‡æœ¬åˆå¹¶ä¸ºä¸€è¡Œï¼Œå»ç©ºæ ¼
            current_text_raw = " ".join(block["text_lines"]).strip()

            # å»é‡é€»è¾‘
            if not final_blocks:
                final_blocks.append((block["time"], current_text_raw))
                prev_text = current_text_raw
                continue

            overlap_len = get_longest_overlap(prev_text, current_text_raw)

            if overlap_len > 0:
                new_text = current_text_raw[overlap_len:].strip()
            else:
                new_text = current_text_raw

            # è¿‡æ»¤æ‰ç©ºçš„æˆ–è€…åªæœ‰æ ‡ç‚¹çš„è¡Œ
            if new_text and len(new_text) > 1:
                final_blocks.append((block["time"], new_text))
                prev_text = new_text

        # å†™å…¥åŸæ–‡ä»¶ï¼ˆè¦†ç›–ï¼‰
        with open(file_path, 'w', encoding='utf-8') as f:
            for index, (time, text) in enumerate(final_blocks, 1):
                f.write(f"{index}\n{time}\n{text}\n\n")

        logger.info(
            f"  âœ… å»é‡å®Œæˆ! åŸå§‹è¡Œæ•°: {len(blocks)} -> æ¸…æ´—å: {len(final_blocks)}")
        return True
    except Exception as e:
        logger.error(f"  âŒ å­—å¹•å»é‡å¤±è´¥: {e}")
        return False


def parse_subtitles(file_content: str) -> Tuple[SubtitleData, str]:
    """
    è§£æ SRT å­—å¹•ï¼Œè¿”å›ç»“æ„åŒ–å­—å¹•åˆ—è¡¨ä¸æ•´åˆæ–‡æœ¬
    """
    if file_content.startswith('\ufeff'):
        file_content = file_content.lstrip('\ufeff')
    if file_content.startswith('WEBVTT'):
        file_content = re.sub(r'WEBVTT.*?\n\n', '',
                              file_content, flags=re.DOTALL)

    blocks = file_content.strip().split('\n\n')
    subtitle_data: SubtitleData = []
    consolidated_lines: List[str] = []
    timestamp_pattern = re.compile(
        r'(\d{2}:\d{2}:\d{2}[,\.]\d{3})\s*-->\s*(\d{2}:\d{2}:\d{2}[,\.]\d{3})')

    for block in blocks:
        lines = block.strip().split('\n')
        if not lines:
            continue

        time_match = None
        dialogue_lines: List[str] = []

        for line in lines:
            line = line.strip()
            if '-->' in line and timestamp_pattern.search(line):
                time_match = timestamp_pattern.search(line)
            elif line.isdigit():
                continue
            else:
                dialogue_lines.append(line)

        if time_match and dialogue_lines:
            start_time = time_match.group(1).replace('.', ',')
            end_time = time_match.group(2).replace('.', ',')
            full_dialogue = ' '.join(dialogue_lines)
            subtitle_data.append({
                'start': start_time,
                'end': end_time,
                'text': full_dialogue
            })
            consolidated_lines.append(full_dialogue)

    consolidated_text = '\n'.join(consolidated_lines)
    return subtitle_data, consolidated_text


def detect_language(content: str, chinese_threshold: float = 0.1) -> str:
    """æ£€æµ‹æ–‡æœ¬ä¸»è¦è¯­è¨€ï¼Œé»˜è®¤ä¸­æ–‡å­—ç¬¦æ¯”ä¾‹â‰¥é˜ˆå€¼è§†ä¸ºä¸­æ–‡"""
    total_chars = len(content)
    if total_chars == 0:
        return "Unknown"

    chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', content))
    chinese_ratio = chinese_chars / total_chars
    language = "Chinese" if chinese_ratio >= chinese_threshold else "English"
    logger.info(f"ğŸŒ æ£€æµ‹è¯­è¨€: {language} (ä¸­æ–‡æ¯”ä¾‹: {chinese_ratio:.2%})")
    return language


def generate_chunk_summary(client, chunk_text: str, current_idx: int,
                           total_chunks: int, model_name: str = PRIMARY_MODEL) -> str:
    """
    è°ƒç”¨ Gemini æ¨¡å‹ç”Ÿæˆå•ä¸ªç‰‡æ®µçš„æ€»ç»“
    """
    if client is None:
        raise RuntimeError("genai client æœªåˆå§‹åŒ–")

    prompt = BASE_SYSTEM_PROMPT.format(
        current=current_idx, total=total_chunks) + "\n\n" + chunk_text

    logger.info(
        f"   >>> LLM æ€»ç»“ç¬¬ {current_idx}/{total_chunks} ç‰‡æ®µï¼Œé•¿åº¦ {len(chunk_text)} å­—")
    response = client.models.generate_content(
        model=model_name,
        contents=prompt,
    )
    return response.text


# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class VideoDownloader:
    """è§†é¢‘å’Œå­—å¹•ä¸‹è½½å™¨ï¼ˆä½¿ç”¨yt-dlpï¼‰"""

    def __init__(self, output_dir: str = "downloads", cookies_file: str = None):
        """
        åˆå§‹åŒ–ä¸‹è½½å™¨

        Args:
            output_dir: ä¸‹è½½æ–‡ä»¶ä¿å­˜ç›®å½•
            cookies_file: Cookies æ–‡ä»¶è·¯å¾„ï¼ˆç”¨äº Bilibili ç­‰éœ€è¦ç™»å½•çš„ç½‘ç«™ï¼‰
        """
        self.output_dir = output_dir
        self.cookies_file = cookies_file
        os.makedirs(output_dir, exist_ok=True)

    def _build_ytdlp_command(self, base_args: List[str]) -> List[str]:
        """
        æ„å»º yt-dlp å‘½ä»¤ï¼Œè‡ªåŠ¨æ·»åŠ  cookies å‚æ•°ï¼ˆå¦‚æœæä¾›ï¼‰

        Args:
            base_args: yt-dlp çš„åŸºç¡€å‚æ•°åˆ—è¡¨ï¼ˆä¸åŒ…å« 'yt-dlp'ï¼‰

        Returns:
            å®Œæ•´çš„å‘½ä»¤åˆ—è¡¨
        """
        cmd = ['yt-dlp']
        if self.cookies_file:
            if os.path.exists(self.cookies_file):
                cmd.extend(['--cookies', self.cookies_file])
                logger.info(f"ğŸª ä½¿ç”¨ cookies æ–‡ä»¶: {self.cookies_file}")
            else:
                logger.warning(
                    f"âš ï¸  Cookies æ–‡ä»¶ä¸å­˜åœ¨: {self.cookies_file}ï¼Œå°†ä¸ä½¿ç”¨ cookies")
        cmd.extend(base_args)
        return cmd

    def download(self, url: str, download_video: bool = True) -> Dict[str, str]:
        """
        ä¸‹è½½è§†é¢‘å’Œå­—å¹•

        Args:
            url: è§†é¢‘é“¾æ¥ï¼ˆYouTube/Bilibiliç­‰ï¼‰
            download_video: æ˜¯å¦ä¸‹è½½è§†é¢‘æ–‡ä»¶

        Returns:
            åŒ…å«è§†é¢‘è·¯å¾„å’Œå­—å¹•è·¯å¾„çš„å­—å…¸
            {
                'video': 'è§†é¢‘æ–‡ä»¶è·¯å¾„',
                'subtitle': 'å­—å¹•æ–‡ä»¶è·¯å¾„' æˆ– None,
                'title': 'è§†é¢‘æ ‡é¢˜'
            }
        """
        logger.info(f"å¼€å§‹ä¸‹è½½: {url}")

        # æ£€æŸ¥yt-dlpæ˜¯å¦å®‰è£…
        try:
            subprocess.run(['yt-dlp', '--version'],
                           capture_output=True, check=True)
        except (subprocess.CalledProcessError, FileNotFoundError):
            logger.error("é”™è¯¯: æœªæ‰¾åˆ° yt-dlpï¼Œè¯·å…ˆå®‰è£…: pip install yt-dlp")
            raise

        # è®¾ç½®è¾“å‡ºæ¨¡æ¿
        video_template = os.path.join(self.output_dir, '%(title)s.%(ext)s')
        subtitle_template = os.path.join(
            self.output_dir, '%(title)s.%(language)s.%(ext)s')

        # é¦–å…ˆè·å–è§†é¢‘ä¿¡æ¯
        info_cmd = self._build_ytdlp_command([
            '--dump-json',
            '--skip-download',
            url
        ])

        raw_video_title = None
        try:
            info_output = subprocess.run(
                info_cmd, capture_output=True, text=True, check=True
            )
            video_info = json.loads(info_output.stdout)
            raw_video_title = video_info.get('title', 'video')
            logger.info(f"ğŸ“¹ æ£€æµ‹åˆ°è§†é¢‘æ ‡é¢˜: {raw_video_title}")
            # æ¸…ç†æ ‡é¢˜ä¸­çš„éæ³•å­—ç¬¦
            video_title = sanitize_filename(raw_video_title)
            logger.info(f"ğŸ“ æ¸…ç†åçš„æ ‡é¢˜: {video_title}")
        except Exception as e:
            logger.warning(f"è·å–è§†é¢‘ä¿¡æ¯å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤æ ‡é¢˜")
            raw_video_title = 'video'
            video_title = 'video'

        chinese_ratio = chinese_char_ratio(raw_video_title)
        prefer_chinese = chinese_ratio > 0.3
        if prefer_chinese:
            logger.info(
                f"ğŸŒ æ£€æµ‹åˆ°è§†é¢‘æ ‡é¢˜ä¸­ä¸­æ–‡æ¯”ä¾‹ {chinese_ratio:.0%}ï¼Œä¼˜å…ˆé€‰æ‹©ä¸­æ–‡å­—å¹•")
        else:
            logger.info(
                f"ğŸŒ ä¸­æ–‡æ¯”ä¾‹ {chinese_ratio:.0%}ï¼Œé»˜è®¤ä¼˜å…ˆè‹±æ–‡å­—å¹•")

        video_path = None
        # æ£€æŸ¥æœ¬åœ°æ˜¯å¦å·²æœ‰è§†é¢‘å’Œå­—å¹•æ–‡ä»¶
        logger.info("æ£€æŸ¥æœ¬åœ°æ˜¯å¦å·²æœ‰è§†é¢‘å’Œå­—å¹•æ–‡ä»¶...")
        existing_video_path = None
        existing_subtitle_path = None

        video_extensions = ['.mp4', '.mkv', '.webm', '.flv', '.avi']
        if download_video:
            # æŸ¥æ‰¾æœ¬åœ°è§†é¢‘æ–‡ä»¶ï¼ˆåŒ¹é…æ ‡é¢˜ï¼‰
            for ext in video_extensions:
                potential_video = os.path.join(
                    self.output_dir, f"{video_title}{ext}")
                if os.path.exists(potential_video) and os.path.getsize(potential_video) > 0:
                    existing_video_path = potential_video
                    logger.info(
                        f"âœ… æ‰¾åˆ°æœ¬åœ°è§†é¢‘æ–‡ä»¶: {os.path.basename(existing_video_path)}")
                    break

            # å¦‚æœç²¾ç¡®åŒ¹é…æ²¡æ‰¾åˆ°ï¼Œå°è¯•æ¨¡ç³ŠåŒ¹é…
            if not existing_video_path:
                title_clean = video_title.replace(' ', '_').replace('/', '_')
                title_lower = video_title.lower()
                title_clean_lower = title_clean.lower()
                # æå–æ ‡é¢˜ä¸­çš„å…³é”®è¯ï¼ˆé•¿åº¦>2çš„å•è¯ï¼‰
                title_words = [w for w in re.split(
                    r'[\s_\-]+', title_lower) if len(w) > 2]

                for f in os.listdir(self.output_dir):
                    if f.endswith(tuple(video_extensions)) and not f.endswith(('.srt', '.vtt')):
                        f_lower = f.lower()
                        # æ£€æŸ¥æ˜¯å¦åŒ…å«å®Œæ•´æ ‡é¢˜æˆ–æ¸…ç†åçš„æ ‡é¢˜
                        if title_lower in f_lower or title_clean_lower in f_lower:
                            potential_path = os.path.join(self.output_dir, f)
                            if os.path.getsize(potential_path) > 0:
                                existing_video_path = potential_path
                                logger.info(
                                    f"âœ… æ‰¾åˆ°æœ¬åœ°è§†é¢‘æ–‡ä»¶ï¼ˆæ¨¡ç³ŠåŒ¹é…ï¼‰: {os.path.basename(existing_video_path)}")
                                break
                        # æˆ–è€…æ£€æŸ¥æ˜¯å¦åŒ…å«æ ‡é¢˜ä¸­çš„å¤šä¸ªå…³é”®è¯ï¼ˆè‡³å°‘2ä¸ªï¼‰
                        elif len(title_words) >= 2:
                            matched_words = sum(
                                1 for word in title_words if word in f_lower)
                            if matched_words >= 2:  # è‡³å°‘åŒ¹é…2ä¸ªå…³é”®è¯
                                potential_path = os.path.join(
                                    self.output_dir, f)
                                if os.path.getsize(potential_path) > 0:
                                    existing_video_path = potential_path
                                    logger.info(
                                        f"âœ… æ‰¾åˆ°æœ¬åœ°è§†é¢‘æ–‡ä»¶ï¼ˆå…³é”®è¯åŒ¹é…ï¼Œ{matched_words}/{len(title_words)}ï¼‰: {os.path.basename(existing_video_path)}")
                                    break

        # æŸ¥æ‰¾æœ¬åœ°å­—å¹•æ–‡ä»¶ï¼ˆå®¹å¿ B ç«™/YouTube æ‰©å±•å‘½åï¼Œä¾‹å¦‚ *.NA.ai-zh.srtï¼‰
        existing_subtitle_path = self._find_local_subtitle_file(video_title)
        if existing_subtitle_path:
            logger.info(
                f"âœ… æ‰¾åˆ°æœ¬åœ°å­—å¹•æ–‡ä»¶: {os.path.basename(existing_subtitle_path)}")

        # æ£€æŸ¥å¯ç”¨å­—å¹•
        subtitle_lang = None
        available_subs = ''
        try:
            sub_cmd = self._build_ytdlp_command([
                '--list-subs',
                '--skip-download',
                url
            ])
            sub_output = subprocess.run(
                sub_cmd, capture_output=True, text=True, check=True
            )
            available_subs = sub_output.stdout

            zh_first_preferences = [
                (r'\bai-zh\b', 'ai-zh', "æ‰¾åˆ°ç®€ä½“ä¸­æ–‡å­—å¹• ai-zh"),
                (r'\b(zh-cn|zh_CN|chinese)\b', 'zh-cn', "æ‰¾åˆ°ç®€ä½“ä¸­æ–‡å­—å¹•"),
                (r'\b(zh-tw|zh_TW)\b', 'zh-tw', "æ‰¾åˆ°ç¹ä½“ä¸­æ–‡å­—å¹•"),
                (r'\bzh\b', 'zh', "æ‰¾åˆ°ä¸­æ–‡å­—å¹•"),
                (r'\bai-en\b', 'ai-en', "æ‰¾åˆ°è‹±æ–‡å­—å¹• ai-en"),
                (r'\b(en|english)\b', 'en', "æ‰¾åˆ°è‹±æ–‡å­—å¹•")
            ]
            en_first_preferences = [
                (r'\bai-en\b', 'ai-en', "æ‰¾åˆ°è‹±æ–‡å­—å¹• ai-en"),
                (r'\b(en|english)\b', 'en', "æ‰¾åˆ°è‹±æ–‡å­—å¹•"),
                (r'\bai-zh\b', 'ai-zh', "æ‰¾åˆ°ç®€ä½“ä¸­æ–‡å­—å¹• ai-zh"),
                (r'\b(zh-cn|zh_CN|chinese)\b', 'zh-cn', "æ‰¾åˆ°ç®€ä½“ä¸­æ–‡å­—å¹•"),
                (r'\b(zh-tw|zh_TW)\b', 'zh-tw', "æ‰¾åˆ°ç¹ä½“ä¸­æ–‡å­—å¹•"),
                (r'\bzh\b', 'zh', "æ‰¾åˆ°ä¸­æ–‡å­—å¹•")
            ]

            lang_preferences = zh_first_preferences if prefer_chinese else en_first_preferences

            for pattern, code, message in lang_preferences:
                if re.search(pattern, available_subs, re.IGNORECASE):
                    subtitle_lang = code
                    logger.info(message)
                    break

            if not subtitle_lang:
                logger.warning("æœªæ‰¾åˆ°ä¸­æ–‡æˆ–è‹±æ–‡å­—å¹•ï¼Œå°†å°è¯•ä¸‹è½½æ‰€æœ‰å¯ç”¨å­—å¹•")
                subtitle_lang = 'all'  # ä¸‹è½½æ‰€æœ‰å­—å¹•ï¼Œåç»­é€‰æ‹©
        except Exception as e:
            logger.warning(f"æ£€æŸ¥å­—å¹•å¤±è´¥: {e}ï¼Œå°†å°è¯•ä¸‹è½½æ‰€æœ‰å­—å¹•")
            subtitle_lang = 'all'

        # å¦‚æœå·²æœ‰æœ¬åœ°è§†é¢‘ï¼Œè·³è¿‡ä¸‹è½½
        if download_video:
            if existing_video_path:
                logger.info("â­ï¸  è·³è¿‡è§†é¢‘ä¸‹è½½ï¼Œä½¿ç”¨æœ¬åœ°æ–‡ä»¶")
                video_path = existing_video_path
            else:
                # ä¸‹è½½è§†é¢‘ï¼ˆæœ€é«˜ç”»è´¨ï¼Œä¸ä¸‹è½½éŸ³é¢‘ï¼Œå› ä¸ºåªç”¨äºæˆªå›¾ï¼‰
                logger.info("æ­£åœ¨ä¸‹è½½è§†é¢‘ï¼ˆæœ€é«˜ç”»è´¨ï¼Œæ— éŸ³é¢‘ï¼Œä»…ç”¨äºæˆªå›¾ï¼‰...")
                # åªä¸‹è½½è§†é¢‘æµï¼ˆæœ€é«˜ç”»è´¨ï¼‰ï¼Œä¸ä¸‹è½½éŸ³é¢‘
                video_cmd = self._build_ytdlp_command([
                    # ä¼˜å…ˆmp4ï¼Œå…¶æ¬¡720p+ï¼Œæœ€åä»»ä½•æœ€é«˜ç”»è´¨è§†é¢‘
                    '-f', 'bestvideo[ext=mp4]/bestvideo[height>=720]/bestvideo',
                    '--no-write-subs',  # ä¸ä¸‹è½½å­—å¹•ï¼ˆæˆ‘ä»¬ä¼šå•ç‹¬ä¸‹è½½ï¼‰
                    '--no-playlist',  # ä¸ä¸‹è½½æ’­æ”¾åˆ—è¡¨
                    '-o', video_template,
                    url
                ])

                try:
                    result = subprocess.run(
                        video_cmd, check=True, capture_output=True, text=True)
                    # ç­‰å¾…æ–‡ä»¶å†™å…¥å®Œæˆ
                    import time
                    time.sleep(1)
                except subprocess.CalledProcessError as e:
                    # å¦‚æœåªä¸‹è½½è§†é¢‘å¤±è´¥ï¼Œå°è¯•ä¸‹è½½è§†é¢‘+æœ€ä½éŸ³é¢‘
                    logger.warning(
                        f"åªä¸‹è½½è§†é¢‘å¤±è´¥: {e.stderr if hasattr(e, 'stderr') and e.stderr else str(e)}")
                    logger.info("å°è¯•ä¸‹è½½è§†é¢‘+æœ€ä½éŸ³é¢‘...")
                    video_cmd_fallback = self._build_ytdlp_command([
                        '-f', 'bestvideo[ext=mp4]+worstaudio[ext=m4a]/bestvideo+worstaudio',
                        '--no-write-subs',
                        '--no-playlist',
                        '-o', video_template,
                        url
                    ])
                    try:
                        subprocess.run(video_cmd_fallback, check=True,
                                       capture_output=True, text=True)
                        import time
                        time.sleep(1)
                    except Exception as e2:
                        logger.error(f"è§†é¢‘ä¸‹è½½å¤±è´¥: {e2}")
                        raise

                # æŸ¥æ‰¾ä¸‹è½½çš„è§†é¢‘æ–‡ä»¶ï¼ˆä¼˜å…ˆåŒ¹é…å½“å‰è§†é¢‘æ ‡é¢˜ï¼‰
                video_path = None
                title_clean = video_title.replace(' ', '_').replace('/', '_')

                # 1. ä¼˜å…ˆç²¾ç¡®åŒ¹é…ï¼šæ ‡é¢˜+æ‰©å±•å
                for ext in ['.mp4', '.mkv', '.webm', '.flv', '.avi']:
                    potential_video = os.path.join(
                        self.output_dir, f"{video_title}{ext}")
                    if os.path.exists(potential_video) and os.path.getsize(potential_video) > 0:
                        video_path = potential_video
                        logger.info(
                            f"âœ… æ‰¾åˆ°ä¸‹è½½çš„è§†é¢‘æ–‡ä»¶ï¼ˆç²¾ç¡®åŒ¹é…ï¼‰: {os.path.basename(video_path)}")
                        break

                # 2. å¦‚æœç²¾ç¡®åŒ¹é…æ²¡æ‰¾åˆ°ï¼Œå°è¯•æ¨¡ç³ŠåŒ¹é…å½“å‰è§†é¢‘æ ‡é¢˜
                if not video_path:
                    matching_files = []
                    for f in os.listdir(self.output_dir):
                        if f.endswith(('.mp4', '.mkv', '.webm', '.flv', '.avi')) and not f.endswith(('.srt', '.vtt')):
                            f_lower = f.lower()
                            title_lower = video_title.lower()
                            title_clean_lower = title_clean.lower()
                            # æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦åŒ…å«è§†é¢‘æ ‡é¢˜
                            if (title_lower in f_lower or title_clean_lower in f_lower or
                                    any(part for part in title_lower.split() if len(part) > 3 and part in f_lower)):
                                matching_files.append(f)

                    if matching_files:
                        # é€‰æ‹©åŒ¹é…æ–‡ä»¶ä¸­æœ€å¤§çš„ï¼ˆé€šå¸¸æ˜¯åˆšä¸‹è½½çš„ï¼‰
                        matching_files_with_size = [(f, os.path.getsize(os.path.join(self.output_dir, f)))
                                                    for f in matching_files]
                        matching_files_with_size.sort(
                            key=lambda x: x[1], reverse=True)
                        video_path = os.path.join(
                            self.output_dir, matching_files_with_size[0][0])
                        logger.info(
                            f"âœ… æ‰¾åˆ°ä¸‹è½½çš„è§†é¢‘æ–‡ä»¶ï¼ˆæ¨¡ç³ŠåŒ¹é…ï¼‰: {os.path.basename(video_path)}")

                # 3. å¦‚æœè¿˜æ˜¯æ²¡æ‰¾åˆ°ï¼Œå°è¯•æ‰¾æœ€è¿‘ä¿®æ”¹çš„æ–‡ä»¶ï¼ˆå¯èƒ½æ˜¯åˆšä¸‹è½½çš„ï¼‰
                if not video_path:
                    video_files = []
                    for f in os.listdir(self.output_dir):
                        if f.endswith(('.mp4', '.mkv', '.webm', '.flv', '.avi')) and not f.endswith(('.srt', '.vtt')):
                            video_files.append(f)

                    if video_files:
                        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œé€‰æ‹©æœ€æ–°çš„æ–‡ä»¶
                        video_files_with_time = []
                        for f in video_files:
                            file_path = os.path.join(self.output_dir, f)
                            mtime = os.path.getmtime(file_path)
                            video_files_with_time.append(
                                (f, mtime, os.path.getsize(file_path)))

                        video_files_with_time.sort(
                            key=lambda x: x[1], reverse=True)  # æŒ‰ä¿®æ”¹æ—¶é—´é™åº
                        video_path = os.path.join(
                            self.output_dir, video_files_with_time[0][0])
                        logger.warning(
                            f"âš ï¸  æ— æ³•ç²¾ç¡®åŒ¹é…è§†é¢‘æ ‡é¢˜ï¼Œä½¿ç”¨æœ€è¿‘ä¿®æ”¹çš„æ–‡ä»¶: {os.path.basename(video_path)}")
                        logger.warning(f"   è¯·ç¡®è®¤è¿™æ˜¯æ­£ç¡®çš„è§†é¢‘æ–‡ä»¶ï¼")

                if not video_path:
                    raise FileNotFoundError(f"æœªæ‰¾åˆ°ä¸‹è½½çš„è§†é¢‘æ–‡ä»¶ï¼ˆæ ‡é¢˜: {video_title}ï¼‰")

                file_size = os.path.getsize(video_path) / 1024 / 1024
                logger.info(
                    f"è§†é¢‘æ–‡ä»¶: {os.path.basename(video_path)} ({file_size:.2f} MB)")
        else:
            logger.info("ğŸ“ Non videoæ¨¡å¼ï¼šè·³è¿‡è§†é¢‘ä¸‹è½½")

        # å¦‚æœå·²æœ‰æœ¬åœ°å­—å¹•ï¼Œè·³è¿‡ä¸‹è½½
        if existing_subtitle_path:
            logger.info("â­ï¸  è·³è¿‡å­—å¹•ä¸‹è½½ï¼Œä½¿ç”¨æœ¬åœ°æ–‡ä»¶")
            subtitle_path = existing_subtitle_path
        else:
            # ä¸‹è½½å­—å¹•ï¼ˆå¦‚æœå¯ç”¨ï¼Œåªä¸‹è½½srtæ ¼å¼ï¼‰
            subtitle_path = None
            if subtitle_lang and subtitle_lang != 'all':
                logger.info(f"æ­£åœ¨ä¸‹è½½å­—å¹• ({subtitle_lang})ï¼Œä»…SRTæ ¼å¼...")
                subtitle_cmd = self._build_ytdlp_command([
                    '--write-subs',
                    '--write-auto-subs',  # ä¹Ÿä¸‹è½½è‡ªåŠ¨ç”Ÿæˆçš„å­—å¹•
                    '--sub-langs', subtitle_lang,
                    '--sub-format', 'srt',  # åªä¸‹è½½srtæ ¼å¼
                    '--skip-download',
                    '-o', subtitle_template,
                    url
                ])

                try:
                    subprocess.run(subtitle_cmd, check=True,
                                   capture_output=True)
                except Exception as e:
                    logger.warning(f"å­—å¹•ä¸‹è½½å¤±è´¥: {e}")

            # å¦‚æœsubtitle_langæ˜¯'all'ï¼Œå°è¯•ä¸‹è½½æ‰€æœ‰å­—å¹•ï¼ˆä»…srtæ ¼å¼ï¼‰
            if subtitle_lang == 'all':
                logger.info("å°è¯•ä¸‹è½½æ‰€æœ‰å¯ç”¨å­—å¹•ï¼ˆä»…SRTæ ¼å¼ï¼‰...")
                try:
                    subtitle_cmd = self._build_ytdlp_command([
                        '--write-subs',
                        '--write-auto-subs',
                        '--sub-format', 'srt',  # åªä¸‹è½½srtæ ¼å¼
                        '--skip-download',
                        '-o', subtitle_template,
                        url
                    ])
                    subprocess.run(subtitle_cmd, check=True,
                                   capture_output=True)
                except Exception as e:
                    logger.warning(f"ä¸‹è½½æ‰€æœ‰å­—å¹•å¤±è´¥: {e}")

            # æŸ¥æ‰¾å­—å¹•æ–‡ä»¶ï¼ˆä¼˜å…ˆä¸­æ–‡ï¼Œå…¶æ¬¡è‹±æ–‡ï¼Œä»…srtæ ¼å¼ï¼‰
            if subtitle_lang:
                subtitle_path = self._find_local_subtitle_file(video_title)
                if subtitle_path:
                    logger.info(
                        f"é€‰æ‹©å­—å¹•æ–‡ä»¶: {os.path.basename(subtitle_path)}")
                    # å¯¹ä¸‹è½½çš„å­—å¹•è¿›è¡Œå»é‡å¤„ç†
                    if subtitle_path.endswith('.srt'):
                        remove_duplicates_from_srt(subtitle_path)

        return {
            'video': video_path,
            'subtitle': subtitle_path,
            'title': video_title
        }

    @staticmethod
    def _normalize_for_match(text: str) -> str:
        """å°†æ ‡é¢˜/æ–‡ä»¶åæ ‡å‡†åŒ–ï¼Œä¾¿äºæ¨¡ç³ŠåŒ¹é…"""
        text = text.lower()
        text = re.sub(r'\.na', '.', text)  # Bç«™å­—å¹•ä¼šå‡ºç° .NA
        return re.sub(r'[\s_\-\.]+', '', text)

    @staticmethod
    def _subtitle_lang_priority(filename: str) -> int:
        """å­—å¹•è¯­è¨€ä¼˜å…ˆçº§ï¼šai-zh > zh > ai-en > en > other"""
        name = filename.lower()
        if 'ai-zh' in name:
            return 5
        if any(tag in name for tag in ['zh-cn', 'zh_tw', 'zh', 'chinese', 'ä¸­æ–‡', 'cn']):
            return 4
        if 'ai-en' in name:
            return 3
        if any(tag in name for tag in ['en', 'english', 'è‹±æ–‡']):
            return 2
        return 1

    def _find_local_subtitle_file(self, video_title: str) -> str:
        """
        åœ¨è¾“å‡ºç›®å½•ä¸­æŸ¥æ‰¾æœ€åŒ¹é…çš„è§†é¢‘å­—å¹•æ–‡ä»¶ï¼Œå®¹å¿ B ç«™çš„ .NA/è¯­è¨€åç¼€
        """
        normalized_title_full = video_title.lower()
        normalized_title_simple = self._normalize_for_match(video_title)
        title_words = [w for w in re.split(
            r'[\s_\-]+', normalized_title_full) if len(w) > 2]

        best_candidate = None
        best_score = -1

        for filename in os.listdir(self.output_dir):
            if not filename.lower().endswith('.srt'):
                continue
            file_path = os.path.join(self.output_dir, filename)
            if os.path.getsize(file_path) <= 0:
                continue

            fname_lower = filename.lower()
            fname_simple = self._normalize_for_match(
                os.path.splitext(fname_lower)[0])

            match_score = 0
            if normalized_title_full in fname_lower or normalized_title_simple in fname_simple:
                match_score = 2
            else:
                matched_words = sum(
                    1 for word in title_words if word and word in fname_lower)
                if matched_words >= max(1, len(title_words) // 2):
                    match_score = 1

            if match_score == 0:
                continue

            lang_score = self._subtitle_lang_priority(fname_lower)
            total_score = lang_score * 10 + match_score

            if total_score > best_score:
                best_score = total_score
                best_candidate = file_path
            elif total_score == best_score and best_candidate:
                if os.path.getmtime(file_path) > os.path.getmtime(best_candidate):
                    best_candidate = file_path

        return best_candidate


class TimeRangeExtractor:
    """æ—¶é—´æ®µæå–å™¨ - åœ¨æŒ‡å®šæ—¶é—´æ®µå†…æå–è§†é¢‘å¸§"""

    @staticmethod
    def time_str_to_seconds(time_str: str) -> float:
        """
        å°†æ—¶é—´å­—ç¬¦ä¸²è½¬æ¢ä¸ºç§’æ•°
        æ”¯æŒæ ¼å¼: HH:MM:SS,mmm æˆ– HH:MM:SS.mmm
        """
        # æ›¿æ¢é€—å·ä¸ºç‚¹
        time_str = time_str.replace(',', '.')

        # è§£ææ—¶é—´
        parts = time_str.split(':')
        hours = int(parts[0])
        minutes = int(parts[1])
        seconds = float(parts[2])

        return hours * 3600 + minutes * 60 + seconds

    @staticmethod
    def seconds_to_time_str(seconds: float) -> str:
        """å°†ç§’æ•°è½¬æ¢ä¸º HH:MM:SS æ ¼å¼"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        return f"{hours:02d}:{minutes:02d}:{secs:02d}"

    @staticmethod
    def extract_frames_in_range(video_path: str, start_time: float, end_time: float,
                                output_dir: str, interval: float = 2.0,
                                image_format: str = 'jpg', quality: int = 95,
                                skip_similar: bool = True,
                                similarity_threshold: float = 0.95,
                                # æ–°å¢ï¼šåˆ†å— + ä¸»/è¾…å…³é”®å¸§ + å»æŠ–åŠ¨ç›¸å…³å‚æ•°ï¼ˆä¿æŒæœ‰é»˜è®¤å€¼ï¼ŒåŸæœ‰è°ƒç”¨ä¸å—å½±å“ï¼‰
                                use_block_diff: bool = True,
                                primary_change_threshold: float = 0.25,
                                secondary_change_threshold: float = 0.12,
                                min_primary_interval: float = 4.0,
                                min_secondary_interval: float = 2.0,
                                block_grid_rows: int = 4,
                                block_grid_cols: int = 4) -> List[str]:
        """
        åœ¨æŒ‡å®šæ—¶é—´æ®µå†…æå–è§†é¢‘å¸§

        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            start_time: å¼€å§‹æ—¶é—´ï¼ˆç§’ï¼‰
            end_time: ç»“æŸæ—¶é—´ï¼ˆç§’ï¼‰
            output_dir: è¾“å‡ºç›®å½•
            interval: æå–é—´éš”ï¼ˆç§’ï¼‰
            image_format: å›¾ç‰‡æ ¼å¼
            quality: å›¾ç‰‡è´¨é‡
            skip_similar: æ˜¯å¦å¯ç”¨æ™ºèƒ½å»é‡ï¼ˆä¿ç•™ç›¸ä¼¼åœºæ™¯çš„æœ€åä¸€å¸§ï¼‰

        Returns:
            æå–çš„å›¾ç‰‡æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        os.makedirs(output_dir, exist_ok=True)

        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise ValueError(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")

        fps = cap.get(cv2.CAP_PROP_FPS)
        start_frame = int(start_time * fps)
        end_frame = int(end_time * fps)
        frame_interval = max(1, int(fps * interval))

        # è®¾ç½®å›¾ç‰‡ç¼–ç å‚æ•°
        if image_format.lower() == 'jpg' or image_format.lower() == 'jpeg':
            encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), quality]
            ext = 'jpg'
        elif image_format.lower() == 'png':
            encode_param = [int(cv2.IMWRITE_PNG_COMPRESSION), 3]
            ext = 'png'
        else:
            encode_param = []
            ext = image_format.lower()

        extracted_files = []
        frame_count = 0
        extracted_count = 0
        skipped_similar = 0

        def save_frame(pending, timestamp, is_primary: bool = True):
            nonlocal extracted_count
            if pending is None:
                return
            time_str = TimeRangeExtractor.seconds_to_time_str(timestamp)
            # æ ‡è®°ä¸»å…³é”®å¸§ / è¾…åŠ©å¸§ï¼Œæ–¹ä¾¿åç»­äººå·¥æŸ¥çœ‹ï¼ˆå¯¹åç»­æµç¨‹æ— ç ´åæ€§å½±å“ï¼‰
            kind = "main" if is_primary else "aux"
            filename = f"frame_{kind}_{extracted_count:03d}_{time_str.replace(':', '')}.{ext}"
            filepath = os.path.join(output_dir, filename)
            if encode_param:
                cv2.imwrite(filepath, pending, encode_param)
            else:
                cv2.imwrite(filepath, pending)
            extracted_files.append(filepath)
            extracted_count += 1

        # è·³è½¬åˆ°å¼€å§‹ä½ç½®
        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)
        # æ–°é€»è¾‘ï¼šè®°å½•ä¸Šä¸€æ¬¡çœŸæ­£ä¿å­˜çš„å¸§ï¼ˆç”¨äºæ¯”è¾ƒå˜åŒ–ï¼‰ï¼Œè€Œä¸æ˜¯ä»…ä»…â€œæœ€åä¸€å¸§â€
        last_saved_frame = None
        last_saved_time: Optional[float] = None

        while frame_count <= (end_frame - start_frame):
            ret, frame = cap.read()

            if not ret:
                break

            # æ£€æŸ¥æ˜¯å¦è¶…å‡ºç»“æŸæ—¶é—´
            current_time = start_time + (frame_count / fps)
            if current_time > end_time:
                break

            # æŒ‰é—´éš”æå–
            if frame_count % frame_interval == 0:
                # ä¸åšæ™ºèƒ½å»é‡æ—¶ï¼Œä»æŒ‰å›ºå®šé—´éš”ç›´æ¥ä¿å­˜å¸§
                if not skip_similar:
                    save_frame(frame, current_time, is_primary=True)
                else:
                    # åˆ†å— + å»æŠ–åŠ¨ + ä¸»/è¾…å…³é”®å¸§é€»è¾‘
                    # 1) ç¬¬ä¸€å¸§ï¼šæ— è®ºå¦‚ä½•å…ˆä¿å­˜ä¸ºä¸»å…³é”®å¸§ï¼Œä½œä¸ºåŸºå‡†
                    if last_saved_frame is None:
                        save_frame(frame, current_time, is_primary=True)
                        last_saved_frame = frame.copy()
                        last_saved_time = current_time
                    else:
                        # è®¡ç®—å˜åŒ–ç¨‹åº¦ï¼šé»˜è®¤é‡‡ç”¨åˆ†å—ç°åº¦å¹³å‡å·®
                        if use_block_diff:
                            change_score = TimeRangeExtractor._block_change_score(
                                last_saved_frame, frame,
                                grid_rows=block_grid_rows,
                                grid_cols=block_grid_cols
                            )
                            # change_score èŒƒå›´è¿‘ä¼¼åœ¨ [0,1]ï¼Œè¶Šå¤§å˜åŒ–è¶Šæ˜æ˜¾
                        else:
                            # é€€åŒ–ä¸ºæ—§çš„å…¨å±€ç›¸ä¼¼åº¦ï¼Œå†è½¬æˆâ€œå˜åŒ–åˆ†æ•°â€
                            similarity = TimeRangeExtractor._calculate_similarity(
                                last_saved_frame, frame)
                            change_score = 1.0 - similarity

                        # ä¸ä¸Šä¸€æ¬¡å…³é”®å¸§çš„æ—¶é—´é—´éš”ï¼Œä½œä¸ºâ€œå»æŠ–åŠ¨â€çš„æœ€å°é—´éš”
                        time_since_last = (current_time - last_saved_time) if last_saved_time is not None else float(
                            "inf")

                        is_primary = False
                        should_save = False

                        # ä¸»å…³é”®å¸§ï¼šå˜åŒ–è¾ƒå¤§ï¼Œä¸”è·ç¦»ä¸Šä¸€æ¬¡å…³é”®å¸§é—´éš”å¤Ÿé•¿
                        if change_score >= primary_change_threshold and time_since_last >= min_primary_interval:
                            is_primary = True
                            should_save = True
                        # è¾…åŠ©å…³é”®å¸§ï¼šå˜åŒ–ä¸­ç­‰ï¼Œä½†ä¹Ÿéœ€è¦ä¸€å®šæ—¶é—´é—´éš”ï¼Œé¿å…åŒä¸€å†…å®¹å¯†é›†æˆªå›¾
                        elif change_score >= secondary_change_threshold and time_since_last >= min_secondary_interval:
                            is_primary = False
                            should_save = True
                        else:
                            # å˜åŒ–å¤ªå°æˆ–æ—¶é—´é—´éš”å¤ªçŸ­ï¼Œéƒ½è®¤ä¸ºæ˜¯æŠ–åŠ¨/ç»†èŠ‚å˜åŒ–ï¼Œè·³è¿‡
                            skipped_similar += 1

                        if should_save:
                            save_frame(frame, current_time,
                                       is_primary=is_primary)
                            last_saved_frame = frame.copy()
                            last_saved_time = current_time

            frame_count += 1

        cap.release()

        if skip_similar and skipped_similar > 0:
            logger.info(f"    è·³è¿‡ç›¸ä¼¼/æŠ–åŠ¨å¸§: {skipped_similar}")

        return extracted_files

    @staticmethod
    def _calculate_similarity(frame1, frame2) -> float:
        """è®¡ç®—ä¸¤å¸§ä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼ˆä½¿ç”¨ç¼©æ”¾åçš„MSEï¼‰"""
        gray1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)
        gray2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)

        small_size = (128, 72)
        gray1_small = cv2.resize(gray1, small_size)
        gray2_small = cv2.resize(gray2, small_size)

        mse = np.mean(
            (gray1_small.astype(float) - gray2_small.astype(float)) ** 2)
        max_mse = 255.0 ** 2
        similarity = 1.0 - (mse / max_mse)
        return similarity

    @staticmethod
    def _block_change_score(frame1, frame2, grid_rows: int = 4, grid_cols: int = 4) -> float:
        # 1. é¢„å¤„ç†
        gray1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)
        gray2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)

        # ç»Ÿä¸€ç¼©æ”¾åˆ°è¾ƒå°å°ºå¯¸ (ç¡®ä¿èƒ½è¢« grid æ•´é™¤ï¼Œæ–¹ä¾¿ reshape)
        target_w = 256
        target_h = 144

        # ç®€å•çš„é˜²å¾¡æ€§æ£€æŸ¥ï¼Œé˜²æ­¢ grid è®¾ç½®è¿‡å¤§
        if target_h % grid_rows != 0 or target_w % grid_cols != 0:
            # ä¸ºäº†å‘é‡åŒ–æ€§èƒ½ï¼Œå¼ºè¡Œ resize åˆ°èƒ½æ•´é™¤çš„å¤§å°
            target_h = (target_h // grid_rows) * grid_rows
            target_w = (target_w // grid_cols) * grid_cols

        gray1 = cv2.resize(gray1, (target_w, target_h)).astype(np.float32)
        gray2 = cv2.resize(gray2, (target_w, target_h)).astype(np.float32)

        # 2. è®¡ç®—ç»å¯¹å·®å€¼å›¾ (Global Difference Map)
        diff = np.abs(gray1 - gray2) / 255.0

        # 3. å‘é‡åŒ–åˆ†å— (Magic happens here)
        # å°† (H, W) é‡å¡‘ä¸º (GridRows, BlockH, GridCols, BlockW)
        # ç„¶åäº¤æ¢è½´å˜ä¸º (GridRows, GridCols, BlockH, BlockW)
        block_h = target_h // grid_rows
        block_w = target_w // grid_cols

        reshaped = diff.reshape(grid_rows, block_h, grid_cols, block_w)
        # äº¤æ¢è½´ï¼ŒæŠŠå—å†…çš„åƒç´ ç»´åº¦æ”¾åœ¨æœ€å
        reshaped = reshaped.transpose(0, 2, 1, 3)

        # 4. è®¡ç®—æ¯ä¸ªå—çš„å‡å€¼
        # axis=(2, 3) æ„å‘³ç€å¯¹æ¯ä¸ªå—å†…éƒ¨çš„æ‰€æœ‰åƒç´ æ±‚å¹³å‡
        block_scores = reshaped.mean(axis=(2, 3))

        # block_scores ç°åœ¨æ˜¯ä¸€ä¸ª shape ä¸º (rows, cols) çš„çŸ©é˜µ

        # 5. å†³ç­–ç­–ç•¥ï¼š
        # ç­–ç•¥ A: ä»ç„¶è¿”å›å…¨å±€å¹³å‡ (å’Œä½ ä¹‹å‰çš„é€»è¾‘ä¸€æ ·ï¼Œä½†å¿«å¾ˆå¤š)
        # return float(np.mean(block_scores))

        # ç­–ç•¥ B (æ¨è): è¿”å›æœ€å¤§çš„å±€éƒ¨å˜åŒ–ã€‚
        # è¿™æ ·å³ä½¿åªæœ‰ç”»é¢ä¸€è§’å˜äº†ï¼Œåˆ†æ•°ä¹Ÿä¼šå¾ˆé«˜ã€‚
        return float(np.max(block_scores))

class VideoSummaryApp:
    """è§†é¢‘æ€»ç»“åº”ç”¨ä¸»ç±»"""

    def __init__(self, output_dir: str = "output", test_mode: bool = False,
                 text_only: bool = False, cookies_file: str = None):
        """
        åˆå§‹åŒ–åº”ç”¨

        Args:
            output_dir: è¾“å‡ºç›®å½•
            test_mode: æ˜¯å¦å¯ç”¨æµ‹è¯•æ¨¡å¼ï¼ˆä¸è°ƒç”¨LLMï¼Œä»…è¾“å‡ºPromptï¼‰
            text_only: Non videoæ¨¡å¼ï¼Œä»…ç”Ÿæˆæ–‡æœ¬æ€»ç»“
            cookies_file: Cookies æ–‡ä»¶è·¯å¾„ï¼ˆç”¨äº Bilibili ç­‰éœ€è¦ç™»å½•çš„ç½‘ç«™ï¼‰
        """
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)
        self.downloader = VideoDownloader(
            os.path.join(output_dir, "downloads"), cookies_file=cookies_file)
        self.time_extractor = TimeRangeExtractor()
        self.test_mode = test_mode
        self.text_only = text_only

    def process_video(self, url: Optional[str] = None,
                      frame_extraction_interval: float = 2.0,
                      skip_similar_frames: bool = True,
                      local_video: Optional[str] = None,
                      local_subtitle: Optional[str] = None,
                      provided_title: Optional[str] = None) -> Optional[str]:
        """
        å¤„ç†è§†é¢‘ï¼šä¸‹è½½/åŠ è½½ã€è§£æã€æ€»ç»“ã€æå–å¸§ã€ç”Ÿæˆmarkdown

        Args:
            url: è§†é¢‘é“¾æ¥
            frame_extraction_interval: å¸§æå–é—´éš”ï¼ˆç§’ï¼‰
            skip_similar_frames: æ˜¯å¦è·³è¿‡ç›¸ä¼¼å¸§
            local_video: æœ¬åœ°è§†é¢‘æ–‡ä»¶è·¯å¾„
            local_subtitle: æœ¬åœ°å­—å¹•æ–‡ä»¶è·¯å¾„ï¼ˆSRTï¼‰
            provided_title: æ‰‹åŠ¨æŒ‡å®šè¾“å‡ºæ ‡é¢˜

        Returns:
            ç”Ÿæˆçš„markdownæ–‡ä»¶è·¯å¾„ï¼›è‹¥ç¼ºå°‘å­—å¹•æ— æ³•ç»§ç»­åˆ™è¿”å› None
        """
        logger.info("=" * 60)
        logger.info("å¼€å§‹å¤„ç†è§†é¢‘")
        logger.info("=" * 60)

        logger.info("\n[æ­¥éª¤ 1/5] å‡†å¤‡è§†é¢‘å’Œå­—å¹•...")

        download_result: Optional[Dict[str, str]] = None
        video_path: Optional[str] = None
        subtitle_path: Optional[str] = None

        video_title = sanitize_filename(
            provided_title) if provided_title else None

        if local_video:
            if not os.path.isfile(local_video):
                raise FileNotFoundError(f"æœ¬åœ°è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {local_video}")
            video_path = local_video
            logger.info(f"ğŸ—‚ï¸ ä½¿ç”¨æœ¬åœ°è§†é¢‘: {video_path}")
            if url:
                logger.info("ğŸ“¥ å°†ä½¿ç”¨æä¾›çš„ URL ä¸‹è½½å­—å¹•ï¼Œæ­é…æœ¬åœ°è§†é¢‘å¤„ç†")
            if not video_title:
                base = os.path.splitext(os.path.basename(local_video))[0]
                video_title = sanitize_filename(base)

        if local_subtitle:
            if not os.path.isfile(local_subtitle):
                raise FileNotFoundError(f"æœ¬åœ°å­—å¹•æ–‡ä»¶ä¸å­˜åœ¨: {local_subtitle}")
            subtitle_path = local_subtitle
            logger.info(f"ğŸ—‚ï¸ ä½¿ç”¨æœ¬åœ°å­—å¹•: {subtitle_path}")
            if not video_title:
                base = os.path.splitext(os.path.basename(local_subtitle))[0]
                video_title = sanitize_filename(base)

        if not subtitle_path:
            if not url:
                raise ValueError("æœªæä¾›è§†é¢‘é“¾æ¥æˆ–å­—å¹•æ–‡ä»¶ï¼Œæ— æ³•ç»§ç»­")
            need_video_download = (
                not self.text_only and video_path is None
            )
            download_result = self.downloader.download(
                url, download_video=need_video_download)
            video_path = video_path or download_result.get('video')
            subtitle_path = download_result.get('subtitle')
            if not video_title:
                video_title = download_result.get('title', 'video')
        else:
            if url:
                logger.info("âš ï¸ å·²æŒ‡å®šæœ¬åœ°å­—å¹•ï¼Œå°†è·³è¿‡å­—å¹•ä¸‹è½½")

        if not subtitle_path:
            logger.warning("âš ï¸ ä»…è·å–åˆ°è§†é¢‘æ–‡ä»¶ï¼Œæœªæ‰¾åˆ°å­—å¹•ï¼Œç»ˆæ­¢æœ¬æ¬¡å¤„ç†")
            return None

        if not self.text_only:
            if not video_path:
                if not url:
                    raise ValueError("é text-only æ¨¡å¼éœ€è¦æä¾›è§†é¢‘æ–‡ä»¶æˆ–é“¾æ¥")
                if not download_result:
                    download_result = self.downloader.download(
                        url, download_video=True)
                video_path = download_result.get('video')
            if not video_path or not os.path.isfile(video_path):
                raise FileNotFoundError("æœªæ‰¾åˆ°å¯ç”¨çš„è§†é¢‘æ–‡ä»¶ï¼Œæ— æ³•æå–æˆªå›¾")
        else:
            if not video_path and download_result:
                video_path = download_result.get('video')

        if not video_title:
            if video_path:
                video_title = sanitize_filename(
                    os.path.splitext(os.path.basename(video_path))[0])
            elif subtitle_path:
                video_title = sanitize_filename(
                    os.path.splitext(os.path.basename(subtitle_path))[0])
            else:
                video_title = "video"

        # 2. è§£æå­—å¹•
        logger.info("\n[æ­¥éª¤ 2/5] è§£æå­—å¹•...")
        with open(subtitle_path, 'r', encoding='utf-8') as f:
            subtitle_content = f.read()

        subtitle_data, consolidated_text = parse_subtitles(subtitle_content)
        logger.info(f"è§£æå®Œæˆ: å…± {len(subtitle_data)} æ¡å­—å¹•")

        # ä¿å­˜æ–‡ç¨¿åˆ°ä¸´æ—¶æ–‡ä»¶
        temp_text_file = os.path.join(
            self.output_dir, f"{video_title}_transcript.txt")
        with open(temp_text_file, 'w', encoding='utf-8') as f:
            f.write(consolidated_text)
        logger.info(f"æ–‡ç¨¿å·²ä¿å­˜: {temp_text_file}")

        # 3 & 4. AIæ€»ç»“ ä¸ å…³é”®å¸§æå–ï¼ˆå¹¶è¡Œï¼‰
        logger.info("\n[æ­¥éª¤ 3/5] ç”ŸæˆAIæ€»ç»“...")
        if not self.text_only:
            logger.info("\n[æ­¥éª¤ 4/5] æå–å…³é”®å¸§ï¼ˆä¸æ­¥éª¤ 3 å¹¶è¡Œæ‰§è¡Œï¼‰...")
        else:
            logger.info("\n[æ­¥éª¤ 4/5] Non videoæ¨¡å¼ï¼šè·³è¿‡å…³é”®å¸§æå–")

        # æ£€æµ‹è¯­è¨€å¹¶åˆ‡åˆ†æ–‡æœ¬ï¼ˆæŒ‰è¯/å­—æ•°é‡ï¼‰
        language = detect_language(consolidated_text)
        CHUNK_SIZE = 1000 if language == "Chinese" else 500
        OVERLAP = 60 if language == "Chinese" else 50

        chunks = self._split_subtitles_into_chunks(
            subtitle_data, CHUNK_SIZE, OVERLAP)
        chunk_texts = [chunk['text'] for chunk in chunks]

        logger.info(f"æ–‡æœ¬å·²åˆ‡åˆ†ä¸º {len(chunks)} ä¸ªç‰‡æ®µ")
        for idx, chunk in enumerate(chunks, start=1):
            word_count = self._count_words(chunk['text'])
            logger.info(f"  - ç‰‡æ®µ {idx}/{len(chunks)} è¯æ•°: {word_count}")

        chunk_frames: Dict[int, List[str]] = {}
        if self.text_only:
            summary_path = self._generate_summary_with_chunks(
                temp_text_file, chunk_texts, video_title)
        else:
            frames_dir = os.path.join(
                self.output_dir, f"{video_title}_frames")
            os.makedirs(frames_dir, exist_ok=True)

            with ThreadPoolExecutor(max_workers=2) as executor:
                summary_future = executor.submit(
                    self._generate_summary_with_chunks,
                    temp_text_file, chunk_texts, video_title
                )
                frames_future = executor.submit(
                    self._extract_frames_for_chunks,
                    video_path, chunks, frames_dir,
                    frame_extraction_interval, skip_similar_frames
                )
                summary_path = summary_future.result()
                chunk_frames = frames_future.result()

        # 5. ç”Ÿæˆæœ€ç»ˆmarkdown
        logger.info("\n[æ­¥éª¤ 5/5] ç”Ÿæˆæœ€ç»ˆmarkdownæ–‡æ¡£...")
        # è·å–åŸå§‹æ–‡ä»¶åï¼ˆç”¨äºè¾“å‡ºæ–‡ä»¶åå’Œæ ‡é¢˜ï¼‰
        original_filename = None
        if video_path:
            original_filename = os.path.splitext(
                os.path.basename(video_path))[0]
        elif subtitle_path:
            original_filename = os.path.splitext(
                os.path.basename(subtitle_path))[0]
        else:
            original_filename = video_title

        final_md_path = self._generate_final_markdown(
            summary_path, chunk_texts, chunk_frames, video_title, video_path, original_filename
        )

        # æ¸…ç†ä¸­é—´äº§ç‰©
        self._cleanup_temp_files([temp_text_file, summary_path])

        logger.info("=" * 60)
        logger.info("âœ… å¤„ç†å®Œæˆï¼")
        logger.info(f"ğŸ“„ æœ€ç»ˆæ–‡æ¡£: {final_md_path}")
        logger.info("=" * 60)

        return final_md_path

    @staticmethod
    def _cleanup_temp_files(paths: List[str]) -> None:
        """
        åˆ é™¤ç”Ÿæˆè¿‡ç¨‹ä¸­çš„ä¸´æ—¶æ–‡ä»¶ï¼Œå¿½ç•¥ä¸å­˜åœ¨çš„è·¯å¾„
        """
        for path in paths:
            if not path:
                continue
            try:
                if os.path.exists(path):
                    os.remove(path)
                    logger.info(f"ğŸ§¹ å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {path}")
            except Exception as exc:
                logger.warning(f"æ— æ³•åˆ é™¤ä¸´æ—¶æ–‡ä»¶ {path}: {exc}")

    def _split_subtitles_into_chunks(self, subtitle_data: SubtitleData,
                                     chunk_size: int, overlap: int) -> List[Dict[str, Any]]:
        """
        åŸºäºå­—å¹•æ•°æ®æŒ‰è¯æ•°åˆ‡åˆ†ï¼Œå¹¶è¿”å›æ¯æ®µçš„æ–‡æœ¬å’Œæ—¶é—´èŒƒå›´
        """
        if not subtitle_data:
            return []

        token_pattern = re.compile(r'[\u4e00-\u9fff]|[a-zA-Z0-9]+')
        entry_token_counts = []
        for entry in subtitle_data:
            tokens = token_pattern.findall(entry['text'])
            entry_token_counts.append(max(1, len(tokens)))

        cumulative = [0]
        for count in entry_token_counts:
            cumulative.append(cumulative[-1] + count)

        chunks = []
        start_idx = 0
        total_entries = len(subtitle_data)

        while start_idx < total_entries:
            start_tokens = cumulative[start_idx]
            target_tokens = start_tokens + chunk_size
            end_idx = bisect_left(cumulative, target_tokens, lo=start_idx + 1)
            if end_idx <= start_idx:
                end_idx = start_idx + 1
            if end_idx > total_entries:
                end_idx = total_entries

            chunk_entries = subtitle_data[start_idx:end_idx]
            chunk_text_parts = [
                entry['text'].strip() for entry in chunk_entries if entry['text'].strip()]
            chunk_text = "\n".join(chunk_text_parts).strip()

            start_time = TimeRangeExtractor.time_str_to_seconds(
                chunk_entries[0]['start'])
            end_time = TimeRangeExtractor.time_str_to_seconds(
                chunk_entries[-1]['end'])

            chunks.append({
                'text': chunk_text,
                'start_time': start_time,
                'end_time': end_time,
                'start_index': start_idx,
                'end_index': end_idx
            })

            if end_idx >= total_entries:
                break

            next_tokens = max(0, cumulative[end_idx] - overlap)
            start_idx = bisect_left(cumulative, next_tokens)
            if start_idx >= total_entries:
                break
            # ç¡®ä¿è‡³å°‘å‘å‰æ¨è¿›
            if start_idx == end_idx:
                start_idx += 1

        return chunks

    def _generate_summary_with_chunks(self, text_file: str, chunks: List[str],
                                      video_title: str) -> str:
        """
        ç”Ÿæˆæ€»ç»“å¹¶è¿”å›æ€»ç»“æ–‡ä»¶è·¯å¾„
        è¿™é‡Œéœ€è¦è°ƒç”¨Summary.pyçš„åŠŸèƒ½ï¼Œä½†éœ€è¦è·å–æ¯ä¸ªchunkçš„æ€»ç»“
        """
        # ç”Ÿæˆæ¯ä¸ªchunkçš„æ€»ç»“
        summaries = []
        total_chunks = len(chunks)

        if self.test_mode:
            logger.info("ğŸ”§ æµ‹è¯•æ¨¡å¼å¼€å¯ï¼šä¸ä¼šè°ƒç”¨LLMï¼Œç›´æ¥è¾“å‡ºPromptå†…å®¹")
            for i, chunk in enumerate(chunks):
                current_idx = i + 1
                prompt = BASE_SYSTEM_PROMPT.format(
                    current=current_idx, total=total_chunks) + "\n\n" + chunk
                summaries.append(prompt)
        else:
            # æ£€æŸ¥API KEY
            api_key = os.environ.get("GEMINI_API_KEY", GEMINI_API_KEY)
            if not api_key or "YOUR_API_KEY" in api_key:
                raise ValueError("GEMINI_API_KEY æœªè®¾ç½®ï¼Œè¯·é…ç½®ç¯å¢ƒå˜é‡æˆ–åœ¨ä»£ç ä¸­å¡«å†™ã€‚")
            if genai is None:
                raise ImportError(
                    "æœªæ‰¾åˆ° google-genaiï¼Œè¯·å…ˆå®‰è£…: pip install google-genai")

            try:
                client = genai.Client(api_key=api_key, http_options={
                    'api_version': 'v1alpha'})
            except Exception as e:
                logger.error(f"åˆå§‹åŒ–APIå®¢æˆ·ç«¯å¤±è´¥: {e}")
                raise

            for i, chunk in enumerate(chunks):
                current_idx = i + 1

                logger.info(f"  æ€»ç»“ç‰‡æ®µ {current_idx}/{total_chunks}...")

                max_retries = 5
                for attempt in range(1, max_retries + 1):
                    try:
                        summary = generate_chunk_summary(
                            client, chunk, current_idx, total_chunks, PRIMARY_MODEL
                        )
                        if summary and summary.strip():
                            summaries.append(summary)
                            break
                        else:
                            raise RuntimeError("LLM è¿”å›ç©ºæ€»ç»“")
                    except Exception as e:
                        if attempt < max_retries:
                            logger.warning(
                                f"  ç‰‡æ®µ {current_idx} ç¬¬ {attempt}/{max_retries} æ¬¡æ€»ç»“å¤±è´¥ï¼Œå°†åœ¨ 30 ç§’åé‡è¯•: {e}")
                            time.sleep(30)
                        else:
                            logger.error(
                                f"  ç‰‡æ®µ {current_idx} è¿ç»­ {max_retries} æ¬¡æ€»ç»“å¤±è´¥ï¼Œç»ˆæ­¢å½“å‰æ–‡ä»¶å¤„ç†: {e}")
                            # ç›´æ¥æŠ›å‡ºå¼‚å¸¸ï¼Œä¸­æ­¢å½“å‰æ–‡ä»¶çš„åç»­å¤„ç†
                            raise RuntimeError(
                                f"ç‰‡æ®µ {current_idx} æ€»ç»“åœ¨é‡è¯• {max_retries} æ¬¡åä»å¤±è´¥ï¼Œç»ˆæ­¢æœ¬æ–‡ä»¶å¤„ç†") from e

        # ä¿å­˜æ€»ç»“åˆ°æ–‡ä»¶
        summary_path = os.path.join(
            self.output_dir, f"{video_title}_summary_temp.md")
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write(f"> ç”± AI ç”Ÿæˆï¼Œå…± {len(chunks)} éƒ¨åˆ†\n\n")

            for i, summary in enumerate(summaries):
                f.write(f"\n## ç¬¬ {i+1} éƒ¨åˆ†\n\n")
                f.write(summary)
                f.write("\n\n---\n")

        return summary_path

    def _extract_frames_for_chunks(self, video_path: str,
                                   chunks: List[Dict[str, Any]],
                                   frames_dir: str,
                                   frame_extraction_interval: float,
                                   skip_similar_frames: bool) -> Dict[int, List[str]]:
        """
        ä¸ºæ¯ä¸ªç‰‡æ®µæå–å¸§ï¼Œè¿”å›ç‰‡æ®µç´¢å¼•åˆ°å¸§è·¯å¾„åˆ—è¡¨çš„æ˜ å°„
        """
        chunk_frames: Dict[int, List[str]] = {}

        for i, chunk in enumerate(chunks):
            start_time = chunk['start_time']
            end_time = chunk['end_time']
            time_str = f"{int(start_time//60):02d}m{int(start_time % 60):02d}s-{int(end_time//60):02d}m{int(end_time % 60):02d}s"
            chunk_dir_name = sanitize_filename(
                f"chunk_{i+1:02d}_{time_str}")
            chunk_frames_dir = os.path.join(frames_dir, chunk_dir_name)

            # æµ‹è¯•æ¨¡å¼ä¸‹å¦‚æœç›®å½•å·²å­˜åœ¨åˆ™ç›´æ¥å¤ç”¨ï¼Œé¿å…é‡æ–°æå–
            if self.test_mode and os.path.isdir(chunk_frames_dir):
                logger.info(
                    f"  ç‰‡æ®µ {i+1}/{len(chunks)}: {time_str} -> ä½¿ç”¨ç°æœ‰å¸§ç›®å½•ï¼Œè·³è¿‡æå–")
                existing_files = [
                    os.path.join(chunk_frames_dir, f)
                    for f in sorted(os.listdir(chunk_frames_dir))
                    if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))
                ]
                deduped = self._deduplicate_frame_paths(existing_files)
                chunk_frames[i] = deduped
                logger.info(
                    f"    å¤ç”¨ {len(deduped)} å¸§ï¼ˆå»é‡å‰ {len(existing_files)}ï¼‰")
                continue

            logger.info(f"  ç‰‡æ®µ {i+1}/{len(chunks)}: {time_str} -> æå–å¸§...")
            frame_files = self.time_extractor.extract_frames_in_range(
                video_path, start_time, end_time,
                chunk_frames_dir,
                interval=frame_extraction_interval,
                skip_similar=skip_similar_frames
            )
            deduped_files = self._deduplicate_frame_paths(frame_files)
            chunk_frames[i] = deduped_files
            if len(deduped_files) != len(frame_files):
                logger.info(
                    f"    æå– {len(frame_files)} å¸§ï¼Œå»é‡åä¿ç•™ {len(deduped_files)}")
            else:
                logger.info(f"    æå–äº† {len(frame_files)} å¸§")

        return chunk_frames

    def _generate_final_markdown(self, summary_path: str, chunks: List[str],
                                 chunk_frames: Dict[int, List[str]],
                                 video_title: str, video_path: str, original_filename: str) -> str:
        """
        ç”Ÿæˆæœ€ç»ˆçš„markdownæ–‡æ¡£ï¼ŒåŒ…å«æ€»ç»“å’Œæˆªå›¾
        
        Args:
            summary_path: æ€»ç»“ä¸´æ—¶æ–‡ä»¶è·¯å¾„
            chunks: ç‰‡æ®µæ–‡æœ¬åˆ—è¡¨
            chunk_frames: ç‰‡æ®µç´¢å¼•åˆ°å¸§è·¯å¾„åˆ—è¡¨çš„æ˜ å°„
            video_title: å¤„ç†åçš„è§†é¢‘æ ‡é¢˜ï¼ˆç”¨äºå†…éƒ¨æ ‡è¯†ï¼‰
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„ï¼ˆå¯èƒ½ä¸ºNoneï¼‰
            original_filename: åŸå§‹æ–‡ä»¶åï¼ˆç”¨äºè¾“å‡ºæ–‡ä»¶åå’Œä¸€çº§æ ‡é¢˜ï¼‰
        """
        # è¯»å–æ€»ç»“å†…å®¹
        with open(summary_path, 'r', encoding='utf-8') as f:
            summary_content = f.read()

        # ä½¿ç”¨åŸå§‹æ–‡ä»¶åä½œä¸ºè¾“å‡ºæ–‡ä»¶å
        final_md_path = os.path.join(
            self.output_dir, f"{original_filename}.md")

        with open(final_md_path, 'w', encoding='utf-8') as f:
            # å†™å…¥ä¸€çº§æ ‡é¢˜ï¼ˆåŸå§‹æ–‡ä»¶åï¼‰
            f.write(f"# {original_filename}\n\n")

            # è§£ææ€»ç»“ï¼Œæ‰¾åˆ°æ¯ä¸ªéƒ¨åˆ†
            # ä½¿ç”¨æ›´çµæ´»çš„æ–¹å¼åˆ†å‰²å†…å®¹
            parts = re.split(r'\n## ç¬¬ (\d+) éƒ¨åˆ†\n', summary_content)

            # å¦‚æœåˆ†å‰²æˆåŠŸï¼Œpartsåº”è¯¥æ˜¯: [æ ‡é¢˜å’Œå¼€å¤´å†…å®¹, '1', ç¬¬ä¸€éƒ¨åˆ†å†…å®¹, '2', ç¬¬äºŒéƒ¨åˆ†å†…å®¹, ...]
            # parts[0] åŒ…å« "> ç”± AI ç”Ÿæˆï¼Œå…± X éƒ¨åˆ†\n\n"ï¼Œæˆ‘ä»¬ç›´æ¥è·³è¿‡å®ƒ
            if len(parts) > 1:

                # å¤„ç†æ¯ä¸ªéƒ¨åˆ†
                for i in range(1, len(parts), 2):
                    if i + 1 >= len(parts):
                        break

                    part_num = parts[i]
                    part_content = parts[i + 1]

                    try:
                        chunk_idx = int(part_num) - 1
                    except ValueError:
                        continue

                    # å†™å…¥éƒ¨åˆ†æ ‡é¢˜
                    f.write(f"\n## ç¬¬ {part_num} éƒ¨åˆ†\n\n")

                    frames_for_chunk = chunk_frames.get(chunk_idx, [])

                    # å†å†™æ€»ç»“å†…å®¹ï¼ˆå»é™¤æœ«å°¾çš„---åˆ†éš”ç¬¦ï¼‰
                    part_content_clean = part_content.rstrip(
                        '\n').rstrip('---').rstrip('\n').strip()

                    sections = [
                        s.strip() for s in re.split(
                            r'\n\s*---+\s*\n', part_content_clean)
                        if s.strip()
                    ]

                    inserted_by_section = False
                    if frames_for_chunk and len(sections) > 1:
                        allocations = self._allocate_frame_counts(
                            sections, len(frames_for_chunk))
                        frame_cursor = 0
                        for section_idx, section_text in enumerate(sections):
                            num_frames = allocations[section_idx] if section_idx < len(
                                allocations) else 0
                            if num_frames > 0:
                                section_frames = frames_for_chunk[
                                    frame_cursor:frame_cursor + num_frames]
                                self._write_frame_block(
                                    f, section_frames, final_md_path)
                                frame_cursor += num_frames
                            f.write(section_text)
                            f.write("\n\n")
                            if section_idx < len(sections) - 1:
                                f.write("---\n\n")
                        inserted_by_section = True

                    if not inserted_by_section:
                        if frames_for_chunk:
                            self._write_frame_block(
                                f, frames_for_chunk, final_md_path)
                        f.write(part_content_clean)

                    f.write("\n\n---\n\n")
            else:
                # å¦‚æœæ— æ³•åˆ†å‰²ï¼Œç›´æ¥å†™å…¥æ•´ä¸ªå†…å®¹
                f.write(summary_content)

        return final_md_path

    def _write_frame_block(self, file_obj: TextIO, frame_paths: List[str],
                           final_md_path: str) -> None:
        """
        å°†ä¸€ç»„å¸§ä»¥ Markdown å›¾ç‰‡å½¢å¼å†™å…¥
        """
        if not frame_paths:
            return

        # file_obj.write("### ğŸ“¸ ç›¸å…³æˆªå›¾\n\n")
        base_dir = os.path.dirname(final_md_path)
        for frame_path in frame_paths:
            if not os.path.exists(frame_path):
                continue
            try:
                rel_path = os.path.relpath(frame_path, base_dir)
                rel_path = self._format_md_path(rel_path)
                file_obj.write(f"![æˆªå›¾]({rel_path})\n\n")
            except ValueError:
                fallback_path = self._format_md_path(frame_path)
                file_obj.write(f"![æˆªå›¾]({fallback_path})\n\n")

    def _deduplicate_frame_paths(self, frame_paths: List[str],
                                 similarity_threshold: float = 0.97) -> List[str]:
        """
        å¯¹å›¾ç‰‡è·¯å¾„æŒ‰å†…å®¹ç›¸ä¼¼åº¦å»é‡
        """
        if not frame_paths:
            return frame_paths

        deduped: List[str] = []
        reference_images: List[np.ndarray] = []
        for path in frame_paths:
            if not os.path.exists(path):
                continue
            image = cv2.imread(path)
            if image is None:
                continue
            is_duplicate = False
            for ref_img in reference_images:
                similarity = TimeRangeExtractor._calculate_similarity(
                    ref_img, image)
                if similarity >= similarity_threshold:
                    is_duplicate = True
                    break

            if is_duplicate:
                continue

            deduped.append(path)
            reference_images.append(image)

        if len(deduped) != len(frame_paths):
            logger.info(
                f"    å»é‡åä¿ç•™ {len(deduped)}/{len(frame_paths)} å¸§")
        return deduped

    def _allocate_frame_counts(self, sections: List[str],
                               total_frames: int) -> List[int]:
        """
        æ ¹æ®æ¯ä¸ªæ®µè½çš„å­—æ•°æŒ‰æ¯”ä¾‹åˆ†é…æˆªå›¾æ•°é‡
        """
        if total_frames <= 0:
            return [0] * len(sections)

        weights: List[int] = []
        for section in sections:
            weight = self._count_words(section)
            weights.append(weight if weight > 0 else 1)

        total_weight = sum(weights)
        if total_weight == 0:
            total_weight = len(sections)
            weights = [1] * len(sections)

        allocations: List[int] = []
        remainders: List[float] = []
        assigned = 0
        for weight in weights:
            exact = (total_frames * weight) / total_weight
            alloc = int(exact)
            allocations.append(alloc)
            remainders.append(exact - alloc)
            assigned += alloc

        remaining = total_frames - assigned
        if remaining > 0:
            order = sorted(
                range(len(sections)),
                key=lambda idx: remainders[idx],
                reverse=True
            )
            idx = 0
            while remaining > 0 and order:
                target = order[idx % len(order)]
                allocations[target] += 1
                remaining -= 1
                idx += 1

        return allocations

    @staticmethod
    def _count_words(text: str) -> int:
        """
        ç»Ÿè®¡ä¸­è‹±æ–‡è¯æ•°ï¼šä¸­æ–‡é€å­—è®¡æ•°ï¼Œè‹±æ–‡æŒ‰è¿ç»­å­—æ¯æ•°å­—è®¡æ•°
        """
        tokens = re.findall(r'[\u4e00-\u9fff]|[a-zA-Z0-9]+', text)
        return len(tokens)

    @staticmethod
    def _format_md_path(path: str) -> str:
        """
        å°†æ–‡ä»¶è·¯å¾„è§„èŒƒåŒ–ä¸º Markdown å¯ç”¨çš„ URLï¼Œå¤„ç†ç©ºæ ¼ç­‰ç‰¹æ®Šå­—ç¬¦
        """
        normalized = path.replace(os.sep, '/')
        return quote(normalized, safe="/:-_.()")


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(
        description='è§†é¢‘æ€»ç»“åº”ç”¨ - ä¸‹è½½è§†é¢‘ã€ç”ŸæˆAIæ€»ç»“å¹¶æå–å…³é”®å¸§',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python video_summary_app.py "https://www.youtube.com/watch?v=xxx"
  python video_summary_app.py "https://www.bilibili.com/video/xxx" -o my_output
  python video_summary_app.py "https://www.bilibili.com/video/xxx" -c cookies.txt
  python video_summary_app.py "https://youtube.com/watch?v=xxx" -i 3.0
  
å‚æ•°è¯´æ˜:
  -o: è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤: outputï¼‰
  -i: å¸§æå–é—´éš”ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤: 2.0
  -c: Cookies æ–‡ä»¶è·¯å¾„ï¼ˆç”¨äº Bilibili ç­‰éœ€è¦ç™»å½•çš„ç½‘ç«™ï¼‰
        """
    )

    parser.add_argument('url', nargs='?', default=None,
                        help='è§†é¢‘é“¾æ¥ï¼ˆYouTube/Bilibiliç­‰ï¼‰ï¼Œå¯é€‰ï¼ˆæœ¬åœ°æ¨¡å¼å¯çœç•¥ï¼‰')
    parser.add_argument('-o', '--output', default='output',
                        help='è¾“å‡ºç›®å½•ï¼Œé»˜è®¤: output')
    parser.add_argument('-i', '--interval', type=float, default=2.0,
                        help='å¸§æå–é—´éš”ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤: 2.0')
    parser.add_argument(
        '-t', '--test', action='store_true',
        help='æµ‹è¯•æ¨¡å¼ï¼šä¸è°ƒç”¨LLMï¼Œç›´æ¥æŠŠPromptå†™å…¥è¾“å‡ºï¼Œä¾¿äºæŸ¥çœ‹ä¸Šä¸‹æ–‡')
    parser.add_argument(
        '-n', '--text-only', action='store_true',
        help='Non videoæ¨¡å¼ï¼šä¸ä¸‹è½½è§†é¢‘ã€ä¸æå–æˆªå›¾ï¼Œä»…è¾“å‡ºæ–‡æœ¬æ€»ç»“')
    parser.add_argument(
        '--local-video', type=str, default=None,
        help='æœ¬åœ°è§†é¢‘æ–‡ä»¶è·¯å¾„ï¼ˆé…åˆæœ¬åœ°å­—å¹•æˆ–ä»…æå–å¸§ï¼‰')
    parser.add_argument(
        '--local-subtitle', type=str, default=None,
        help='æœ¬åœ°å­—å¹•æ–‡ä»¶è·¯å¾„ï¼ˆSRTï¼‰ï¼›text-only æ¨¡å¼ä¸‹åªéœ€è¯¥å‚æ•°')
    parser.add_argument(
        '--title', type=str, default=None,
        help='æ‰‹åŠ¨æŒ‡å®šè¾“å‡ºæ ‡é¢˜ï¼ˆå¯é€‰ï¼‰')
    parser.add_argument(
        '-c', '--cookies', type=str, default=None,
        help='Cookies æ–‡ä»¶è·¯å¾„ï¼ˆç”¨äº Bilibili ç­‰éœ€è¦ç™»å½•çš„ç½‘ç«™ï¼‰ï¼Œä¾‹å¦‚: --cookies cookies.txt')

    args = parser.parse_args()

    try:
        # éªŒè¯ cookies æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        cookies_file = args.cookies
        if cookies_file and not os.path.exists(cookies_file):
            logger.warning(f"è­¦å‘Š: Cookies æ–‡ä»¶ä¸å­˜åœ¨: {cookies_file}ï¼Œå°†ä¸ä½¿ç”¨ cookies")
            cookies_file = None
        elif cookies_file:
            logger.info(f"âœ… ä½¿ç”¨ Cookies æ–‡ä»¶: {cookies_file}")

        if not args.url and not args.local_subtitle:
            parser.error("å¿…é¡»æä¾›è§†é¢‘é“¾æ¥æˆ–æœ¬åœ°å­—å¹•æ–‡ä»¶")
        if not args.text_only and not (args.url or args.local_video):
            parser.error("é text-only æ¨¡å¼éœ€è¦è§†é¢‘é“¾æ¥æˆ–æœ¬åœ°è§†é¢‘æ–‡ä»¶")

        app = VideoSummaryApp(output_dir=args.output,
                              test_mode=args.test,
                              text_only=args.text_only,
                              cookies_file=cookies_file)
        result_path = app.process_video(
            args.url,
            frame_extraction_interval=args.interval,
            local_video=args.local_video,
            local_subtitle=args.local_subtitle,
            provided_title=args.title)
        if result_path:
            print(f"\nâœ… å®Œæˆï¼ç»“æœæ–‡ä»¶: {result_path}")
        else:
            logger.info("æœ¬æ¬¡ä»»åŠ¡æœªç”Ÿæˆè¾“å‡ºæ–‡ä»¶ã€‚")
    except Exception as e:
        logger.error(f"å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()
